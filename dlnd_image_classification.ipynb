{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [05:15, 541KB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11119c780>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    labels = np.zeros((len(x), 10))\n",
    "    return labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, *image_shape], name='x') \n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, 10], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def convolution(x_tensor, conv_num_outputs, conv_strides):\n",
    "    # Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
    "    weights = [x_tensor.shape.as_list()[1], x_tensor.shape.as_list()[2], x_tensor.shape.as_list()[3], conv_num_outputs]\n",
    "    weights = tf.Variable(tf.random_normal(weights, stddev=0.1)) # input shape times WxHxD\n",
    "    \n",
    "    bias = tf.Variable(tf.constant(0.05, shape=[conv_num_outputs]))# one for each convolution layer [conv_layer_number_of_elemetns] \n",
    "    # Apply a convolution to x_tensor using weight and conv_strides.\n",
    "    # -We recommend you use same padding, but you're welcome to use any padding.\n",
    "    conv_strides = (1,) + conv_strides + (1,)\n",
    "    layer = tf.nn.conv2d(input=x_tensor,\n",
    "                         filter=weights,\n",
    "                         strides=conv_strides,\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add bias\n",
    "    layer += bias\n",
    "    return layer \n",
    "\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_dict = {'x_tensor':x_tensor,\n",
    "                  'conv_num_outputs':conv_num_outputs,\n",
    "                  'conv_ksize':conv_ksize,\n",
    "                  'conv_strides':conv_strides,\n",
    "                  'pool_ksize':pool_ksize,\n",
    "                  'pool_strides':pool_strides}\n",
    "    \n",
    "    layer = convolution(x_tensor, conv_num_outputs, conv_strides)\n",
    "    \n",
    "    # Add a nonlinear activation to the convolution.\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    # Apply Max Pooling using pool_ksize and pool_strides.\n",
    "    # -We recommend you use same padding, but you're welcome to use any padding.\n",
    "    pool_ksize = (1,) + pool_ksize + (1,)\n",
    "    pool_strides = (1,) + pool_strides + (1,)\n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=pool_ksize,\n",
    "                               strides=pool_strides,\n",
    "                               padding='SAME')\n",
    "    \n",
    "    return layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print('{:16}: {}'.format(\"x_tensor\", x_tensor))\n",
    "    image_size = x_tensor.shape[1:4].num_elements()\n",
    "    result_tuple = np.array([x_tensor.shape.as_list()[0], image_size])\n",
    "#     print('{:16}: {}'.format(\"result_tuple\", result_tuple))\n",
    "    return tf.reshape(x_tensor, [-1, image_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Create new weights and biases.\n",
    "    num_inputs = x_tensor.shape[1:4].num_elements()\n",
    "    weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05)) \n",
    "    biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(x_tensor, weights) + biases\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    layer = fully_conn(x_tensor, num_outputs)\n",
    "    return tf.nn.relu(layer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "#     input_dict = {\"x\":x, \"keep_prob\":keep_prob}\n",
    "#     for key in input_dict:\n",
    "#         print('{:16}: {}'.format(key, input_dict[key]))\n",
    "        \n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv_num_outputs= 10\n",
    "    conv_ksize      = (2, 2)\n",
    "    conv_strides    = (4, 4)\n",
    "    pool_ksize      = (2, 2)\n",
    "    pool_strides    = (2, 2)\n",
    "    \n",
    "    layer = convolution(x, conv_num_outputs, conv_strides)\n",
    "    layer = convolution(layer, conv_num_outputs, conv_strides)\n",
    "    layer = conv2d_maxpool(layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    tf.nn.dropout(layer, keep_prob)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    layer = flatten(layer)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    layer = fully_conn(layer, conv_num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    layer = output(layer, conv_num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "#     print(\"{:16}: {}\".format('session', session))\n",
    "#     print(\"{:16}: {}\".format('optimizer', optimizer))\n",
    "#     print(\"{:16}: {}\".format('keep_probability', keep_probability))\n",
    "#     print(\"{:16}: {}\".format('feature_batch', feature_batch[0][0][0][:3]))\n",
    "#     print(\"{:16}: {}\".format('label_batch', label_batch[0][0]))\n",
    "\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, \n",
    "                                      y:label_batch, \n",
    "                                      keep_prob:keep_probability })\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "def print_stats(session, x_batch, y_true_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    print('')\n",
    "    \n",
    "    feed_dict_train = {\"x:0\": x_batch,\n",
    "                       \"y:0\": y_true_batch,\n",
    "                      'keep_prob:0':1.0}\n",
    "    cost = session.run(cost, feed_dict=feed_dict_train)\n",
    "        \n",
    "    accuracy = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    print('cost:{}'.format(cost))\n",
    "    print('acc:{}'.format(accuracy))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "cost:0.0\n",
      "acc:1.0\n",
      "Epoch  2, CIFAR-10 Batch 1:  \n",
      "cost:0.0\n",
      "acc:1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.88235294  0.93333333  1.        ]\n",
      "   [ 0.90980392  0.94509804  1.        ]\n",
      "   [ 0.9372549   0.96470588  1.        ]\n",
      "   ..., \n",
      "   [ 0.79607843  0.8         0.83137255]\n",
      "   [ 0.8         0.83137255  0.84705882]\n",
      "   [ 0.69411765  0.7254902   0.73333333]]\n",
      "\n",
      "  [[ 0.82352941  0.87843137  0.96078431]\n",
      "   [ 0.88627451  0.92156863  0.98823529]\n",
      "   [ 0.92156863  0.94509804  1.        ]\n",
      "   ..., \n",
      "   [ 0.63921569  0.64313725  0.64313725]\n",
      "   [ 0.58431373  0.6         0.57647059]\n",
      "   [ 0.60392157  0.62352941  0.58823529]]\n",
      "\n",
      "  [[ 0.76470588  0.78039216  0.83921569]\n",
      "   [ 0.81568627  0.81568627  0.85490196]\n",
      "   [ 0.8627451   0.83921569  0.87843137]\n",
      "   ..., \n",
      "   [ 0.55686275  0.56078431  0.5254902 ]\n",
      "   [ 0.45882353  0.46666667  0.4       ]\n",
      "   [ 0.45490196  0.4627451   0.38823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.63921569  0.63529412  0.60392157]\n",
      "   [ 0.64313725  0.61568627  0.59215686]\n",
      "   [ 0.6627451   0.62745098  0.6       ]\n",
      "   ..., \n",
      "   [ 0.67843137  0.65882353  0.61568627]\n",
      "   [ 0.6627451   0.65490196  0.60784314]\n",
      "   [ 0.64705882  0.63921569  0.59215686]]\n",
      "\n",
      "  [[ 0.61176471  0.60392157  0.56470588]\n",
      "   [ 0.64705882  0.61960784  0.6       ]\n",
      "   [ 0.6745098   0.64313725  0.61568627]\n",
      "   ..., \n",
      "   [ 0.64705882  0.63137255  0.58431373]\n",
      "   [ 0.62745098  0.61960784  0.57254902]\n",
      "   [ 0.62745098  0.61960784  0.56862745]]\n",
      "\n",
      "  [[ 0.62745098  0.6         0.56078431]\n",
      "   [ 0.65490196  0.62745098  0.58823529]\n",
      "   [ 0.65490196  0.62745098  0.59215686]\n",
      "   ..., \n",
      "   [ 0.63921569  0.63137255  0.58431373]\n",
      "   [ 0.63529412  0.62745098  0.58039216]\n",
      "   [ 0.63137255  0.62352941  0.57647059]]]\n",
      "\n",
      "\n",
      " [[[ 0.68235294  0.58823529  0.52941176]\n",
      "   [ 0.72156863  0.56470588  0.43529412]\n",
      "   [ 0.74509804  0.62745098  0.50196078]\n",
      "   ..., \n",
      "   [ 0.71764706  0.63137255  0.5372549 ]\n",
      "   [ 0.70588235  0.59607843  0.49019608]\n",
      "   [ 0.69019608  0.59607843  0.49803922]]\n",
      "\n",
      "  [[ 0.65490196  0.58431373  0.56862745]\n",
      "   [ 0.69019608  0.54901961  0.44705882]\n",
      "   [ 0.74117647  0.63137255  0.52156863]\n",
      "   ..., \n",
      "   [ 0.69019608  0.61568627  0.52941176]\n",
      "   [ 0.67843137  0.58039216  0.48235294]\n",
      "   [ 0.6745098   0.59215686  0.50196078]]\n",
      "\n",
      "  [[ 0.69019608  0.61568627  0.6       ]\n",
      "   [ 0.66666667  0.52941176  0.42352941]\n",
      "   [ 0.73333333  0.63137255  0.51372549]\n",
      "   ..., \n",
      "   [ 0.70196078  0.62352941  0.53333333]\n",
      "   [ 0.69411765  0.59215686  0.49019608]\n",
      "   [ 0.69803922  0.61176471  0.51764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.26666667  0.21960784  0.25882353]\n",
      "   [ 0.23137255  0.18039216  0.21960784]\n",
      "   [ 0.41176471  0.40784314  0.43529412]\n",
      "   ..., \n",
      "   [ 0.69803922  0.61568627  0.54509804]\n",
      "   [ 0.68235294  0.6         0.52156863]\n",
      "   [ 0.6627451   0.59215686  0.5254902 ]]\n",
      "\n",
      "  [[ 0.34509804  0.28627451  0.32156863]\n",
      "   [ 0.35294118  0.31764706  0.34509804]\n",
      "   [ 0.51764706  0.54509804  0.55294118]\n",
      "   ..., \n",
      "   [ 0.68627451  0.61568627  0.55686275]\n",
      "   [ 0.68235294  0.61176471  0.54509804]\n",
      "   [ 0.67058824  0.60392157  0.52941176]]\n",
      "\n",
      "  [[ 0.42745098  0.32941176  0.36862745]\n",
      "   [ 0.40784314  0.34509804  0.37254902]\n",
      "   [ 0.56862745  0.58039216  0.58431373]\n",
      "   ..., \n",
      "   [ 0.6745098   0.60392157  0.54509804]\n",
      "   [ 0.67058824  0.60392157  0.5372549 ]\n",
      "   [ 0.67843137  0.61176471  0.5372549 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.27058824  0.27843137  0.20392157]\n",
      "   [ 0.24313725  0.24313725  0.19215686]\n",
      "   [ 0.22745098  0.22352941  0.18823529]\n",
      "   ..., \n",
      "   [ 0.4627451   0.49019608  0.31372549]\n",
      "   [ 0.4745098   0.49019608  0.28627451]\n",
      "   [ 0.48235294  0.50588235  0.29019608]]\n",
      "\n",
      "  [[ 0.2745098   0.27843137  0.19215686]\n",
      "   [ 0.23137255  0.22745098  0.18431373]\n",
      "   [ 0.2         0.19215686  0.16862745]\n",
      "   ..., \n",
      "   [ 0.48235294  0.48235294  0.3254902 ]\n",
      "   [ 0.45882353  0.47843137  0.29803922]\n",
      "   [ 0.41568627  0.43921569  0.25490196]]\n",
      "\n",
      "  [[ 0.28627451  0.28235294  0.18431373]\n",
      "   [ 0.25490196  0.24705882  0.17647059]\n",
      "   [ 0.20392157  0.19607843  0.17254902]\n",
      "   ..., \n",
      "   [ 0.47058824  0.46666667  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.27058824]\n",
      "   [ 0.42745098  0.43529412  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.76862745  0.82352941  0.37254902]\n",
      "   [ 0.79215686  0.82745098  0.42745098]\n",
      "   [ 0.76078431  0.78823529  0.41176471]\n",
      "   ..., \n",
      "   [ 0.77647059  0.84313725  0.35294118]\n",
      "   [ 0.81568627  0.85882353  0.42352941]\n",
      "   [ 0.83137255  0.88235294  0.43529412]]\n",
      "\n",
      "  [[ 0.74509804  0.80784314  0.36862745]\n",
      "   [ 0.74901961  0.80784314  0.37254902]\n",
      "   [ 0.74509804  0.80392157  0.36470588]\n",
      "   ..., \n",
      "   [ 0.77647059  0.83921569  0.36862745]\n",
      "   [ 0.76470588  0.81568627  0.37254902]\n",
      "   [ 0.80784314  0.8627451   0.41960784]]\n",
      "\n",
      "  [[ 0.71764706  0.76470588  0.36470588]\n",
      "   [ 0.72156863  0.77254902  0.37647059]\n",
      "   [ 0.71372549  0.75686275  0.36078431]\n",
      "   ..., \n",
      "   [ 0.75686275  0.81176471  0.35686275]\n",
      "   [ 0.74117647  0.8         0.34117647]\n",
      "   [ 0.77647059  0.82745098  0.39215686]]]\n",
      "\n",
      "\n",
      " [[[ 0.61960784  0.61960784  0.61960784]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61960784  0.62352941  0.60392157]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59215686]]\n",
      "\n",
      "  [[ 0.61568627  0.61568627  0.61568627]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.6       ]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59607843]]\n",
      "\n",
      "  [[ 0.61176471  0.61176471  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.22745098  0.26666667  0.26666667]\n",
      "   [ 0.24313725  0.28235294  0.29019608]\n",
      "   [ 0.21960784  0.25882353  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.24313725  0.2745098   0.30196078]\n",
      "   [ 0.18823529  0.22352941  0.23137255]\n",
      "   [ 0.22352941  0.25882353  0.26666667]]\n",
      "\n",
      "  [[ 0.20392157  0.24313725  0.23921569]\n",
      "   [ 0.23529412  0.27058824  0.27843137]\n",
      "   [ 0.16862745  0.20392157  0.20392157]\n",
      "   ..., \n",
      "   [ 0.25882353  0.29803922  0.30980392]\n",
      "   [ 0.21568627  0.25882353  0.2627451 ]\n",
      "   [ 0.23529412  0.2745098   0.29019608]]\n",
      "\n",
      "  [[ 0.19607843  0.22745098  0.23137255]\n",
      "   [ 0.2         0.21960784  0.21960784]\n",
      "   [ 0.20784314  0.23529412  0.23921569]\n",
      "   ..., \n",
      "   [ 0.18039216  0.21176471  0.21176471]\n",
      "   [ 0.22745098  0.25882353  0.26666667]\n",
      "   [ 0.23921569  0.28235294  0.29019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.76862745  0.67058824  0.51372549]\n",
      "   [ 0.76862745  0.6745098   0.51372549]\n",
      "   [ 0.75686275  0.65882353  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69411765  0.6         0.50980392]\n",
      "   [ 0.67058824  0.57647059  0.48627451]\n",
      "   [ 0.64705882  0.55294118  0.45882353]]\n",
      "\n",
      "  [[ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.76470588  0.6627451   0.50588235]\n",
      "   ..., \n",
      "   [ 0.69803922  0.60784314  0.51764706]\n",
      "   [ 0.6745098   0.58431373  0.49411765]\n",
      "   [ 0.65490196  0.56470588  0.47058824]]\n",
      "\n",
      "  [[ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.75294118  0.65490196  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69019608  0.60392157  0.51764706]\n",
      "   [ 0.66666667  0.58431373  0.49411765]\n",
      "   [ 0.64705882  0.56078431  0.4745098 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.7372549   0.61960784  0.40392157]\n",
      "   [ 0.7372549   0.62745098  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.4       ]\n",
      "   ..., \n",
      "   [ 0.8         0.74509804  0.69803922]\n",
      "   [ 0.78431373  0.74117647  0.70196078]\n",
      "   [ 0.76862745  0.71764706  0.68235294]]\n",
      "\n",
      "  [[ 0.74117647  0.62352941  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.40784314]\n",
      "   [ 0.74509804  0.63137255  0.4       ]\n",
      "   ..., \n",
      "   [ 0.80392157  0.74509804  0.68627451]\n",
      "   [ 0.78431373  0.73333333  0.69019608]\n",
      "   [ 0.77254902  0.71764706  0.67843137]]\n",
      "\n",
      "  [[ 0.72156863  0.60392157  0.39215686]\n",
      "   [ 0.72156863  0.61176471  0.39215686]\n",
      "   [ 0.72156863  0.60784314  0.38039216]\n",
      "   ..., \n",
      "   [ 0.78039216  0.71372549  0.64705882]\n",
      "   [ 0.76862745  0.70588235  0.65490196]\n",
      "   [ 0.75686275  0.69411765  0.65098039]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  1, CIFAR-10 Batch 2:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.8         0.79607843  0.67843137]\n",
      "   [ 0.81568627  0.81960784  0.70980392]\n",
      "   [ 0.83921569  0.84313725  0.7372549 ]\n",
      "   ..., \n",
      "   [ 0.61176471  0.55294118  0.3254902 ]\n",
      "   [ 0.59215686  0.5372549   0.31764706]\n",
      "   [ 0.56862745  0.51764706  0.30588235]]\n",
      "\n",
      "  [[ 0.84705882  0.85098039  0.74901961]\n",
      "   [ 0.8745098   0.88235294  0.79215686]\n",
      "   [ 0.89019608  0.89411765  0.81176471]\n",
      "   ..., \n",
      "   [ 0.68235294  0.63529412  0.38823529]\n",
      "   [ 0.67843137  0.62745098  0.38039216]\n",
      "   [ 0.66666667  0.61176471  0.37254902]]\n",
      "\n",
      "  [[ 0.83921569  0.83137255  0.72941176]\n",
      "   [ 0.85490196  0.85098039  0.75294118]\n",
      "   [ 0.85490196  0.84705882  0.74117647]\n",
      "   ..., \n",
      "   [ 0.7372549   0.68627451  0.44313725]\n",
      "   [ 0.72941176  0.68235294  0.43921569]\n",
      "   [ 0.71372549  0.66666667  0.42352941]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.64313725  0.64705882  0.47843137]\n",
      "   [ 0.65882353  0.66666667  0.49019608]\n",
      "   [ 0.67058824  0.67843137  0.50196078]\n",
      "   ..., \n",
      "   [ 0.36862745  0.26666667  0.14509804]\n",
      "   [ 0.3372549   0.25098039  0.14117647]\n",
      "   [ 0.31372549  0.23921569  0.1372549 ]]\n",
      "\n",
      "  [[ 0.50196078  0.50196078  0.37647059]\n",
      "   [ 0.59607843  0.60392157  0.43921569]\n",
      "   [ 0.63921569  0.65098039  0.46666667]\n",
      "   ..., \n",
      "   [ 0.36470588  0.25882353  0.14117647]\n",
      "   [ 0.33333333  0.24705882  0.13333333]\n",
      "   [ 0.30980392  0.23137255  0.13333333]]\n",
      "\n",
      "  [[ 0.29411765  0.28235294  0.22745098]\n",
      "   [ 0.39607843  0.38431373  0.29803922]\n",
      "   [ 0.50588235  0.50588235  0.37254902]\n",
      "   ..., \n",
      "   [ 0.34901961  0.23921569  0.12941176]\n",
      "   [ 0.32156863  0.22745098  0.1254902 ]\n",
      "   [ 0.29411765  0.21176471  0.1254902 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.8         0.78431373  0.78431373]\n",
      "   [ 0.87843137  0.8627451   0.87058824]\n",
      "   [ 0.86666667  0.84705882  0.87058824]\n",
      "   ..., \n",
      "   [ 0.78039216  0.80392157  0.79215686]\n",
      "   [ 0.76470588  0.79215686  0.76862745]\n",
      "   [ 0.38823529  0.41568627  0.38431373]]\n",
      "\n",
      "  [[ 0.57254902  0.55686275  0.55686275]\n",
      "   [ 0.74901961  0.73333333  0.74117647]\n",
      "   [ 0.83529412  0.81568627  0.83921569]\n",
      "   ..., \n",
      "   [ 0.83921569  0.85882353  0.86666667]\n",
      "   [ 0.76470588  0.8         0.78823529]\n",
      "   [ 0.42745098  0.49803922  0.45490196]]\n",
      "\n",
      "  [[ 0.52941176  0.51372549  0.51372549]\n",
      "   [ 0.71372549  0.69411765  0.70196078]\n",
      "   [ 0.84705882  0.82745098  0.84705882]\n",
      "   ..., \n",
      "   [ 0.83529412  0.85098039  0.8745098 ]\n",
      "   [ 0.75294118  0.78823529  0.79607843]\n",
      "   [ 0.44705882  0.51764706  0.50196078]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.42745098  0.51372549  0.56470588]\n",
      "   [ 0.44313725  0.54117647  0.59215686]\n",
      "   [ 0.43921569  0.54117647  0.59215686]\n",
      "   ..., \n",
      "   [ 0.42745098  0.54117647  0.58431373]\n",
      "   [ 0.41568627  0.53333333  0.57647059]\n",
      "   [ 0.40784314  0.5254902   0.56862745]]\n",
      "\n",
      "  [[ 0.40784314  0.49803922  0.54901961]\n",
      "   [ 0.41960784  0.51764706  0.56862745]\n",
      "   [ 0.42745098  0.52941176  0.58039216]\n",
      "   ..., \n",
      "   [ 0.41960784  0.53333333  0.58039216]\n",
      "   [ 0.4         0.51764706  0.56078431]\n",
      "   [ 0.37647059  0.49411765  0.5372549 ]]\n",
      "\n",
      "  [[ 0.36470588  0.45490196  0.50980392]\n",
      "   [ 0.38823529  0.48627451  0.5372549 ]\n",
      "   [ 0.39607843  0.50196078  0.55294118]\n",
      "   ..., \n",
      "   [ 0.39215686  0.50588235  0.55294118]\n",
      "   [ 0.38823529  0.50588235  0.55294118]\n",
      "   [ 0.36862745  0.49019608  0.53333333]]]\n",
      "\n",
      "\n",
      " [[[ 0.41568627  0.3372549   0.61176471]\n",
      "   [ 0.48235294  0.4         0.50588235]\n",
      "   [ 0.43529412  0.33333333  0.49019608]\n",
      "   ..., \n",
      "   [ 0.51372549  0.4627451   0.61568627]\n",
      "   [ 0.42745098  0.39215686  0.55686275]\n",
      "   [ 0.55294118  0.52941176  0.61960784]]\n",
      "\n",
      "  [[ 0.60784314  0.5254902   0.63529412]\n",
      "   [ 0.6745098   0.57254902  0.44313725]\n",
      "   [ 0.51372549  0.43529412  0.48627451]\n",
      "   ..., \n",
      "   [ 0.51764706  0.46666667  0.60784314]\n",
      "   [ 0.39215686  0.34901961  0.50980392]\n",
      "   [ 0.56862745  0.5372549   0.63529412]]\n",
      "\n",
      "  [[ 0.63529412  0.5254902   0.68235294]\n",
      "   [ 0.62745098  0.49803922  0.43137255]\n",
      "   [ 0.44313725  0.34509804  0.5254902 ]\n",
      "   ..., \n",
      "   [ 0.73333333  0.71372549  0.65490196]\n",
      "   [ 0.69411765  0.6745098   0.65882353]\n",
      "   [ 0.78823529  0.77254902  0.71764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35686275  0.03921569  0.5254902 ]\n",
      "   [ 0.34509804  0.03529412  0.49803922]\n",
      "   [ 0.34509804  0.05098039  0.49803922]\n",
      "   ..., \n",
      "   [ 0.40784314  0.24705882  0.49411765]\n",
      "   [ 0.43529412  0.28235294  0.52156863]\n",
      "   [ 0.43921569  0.25490196  0.53333333]]\n",
      "\n",
      "  [[ 0.29803922  0.02352941  0.50196078]\n",
      "   [ 0.30588235  0.01568627  0.47058824]\n",
      "   [ 0.35686275  0.08235294  0.51764706]\n",
      "   ..., \n",
      "   [ 0.42745098  0.27058824  0.49803922]\n",
      "   [ 0.41960784  0.26666667  0.50980392]\n",
      "   [ 0.44705882  0.24705882  0.54509804]]\n",
      "\n",
      "  [[ 0.2627451   0.03137255  0.50196078]\n",
      "   [ 0.26666667  0.02352941  0.47058824]\n",
      "   [ 0.30980392  0.07058824  0.49803922]\n",
      "   ..., \n",
      "   [ 0.45882353  0.28627451  0.5254902 ]\n",
      "   [ 0.43137255  0.25098039  0.52156863]\n",
      "   [ 0.38823529  0.16470588  0.49019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.96862745  0.96470588  0.97254902]\n",
      "   [ 0.96078431  0.97647059  0.98823529]\n",
      "   [ 0.95686275  0.97254902  0.97647059]\n",
      "   ..., \n",
      "   [ 0.45882353  0.38039216  0.32941176]\n",
      "   [ 0.49803922  0.41960784  0.37647059]\n",
      "   [ 0.61960784  0.56470588  0.54117647]]\n",
      "\n",
      "  [[ 0.95294118  0.95686275  0.96862745]\n",
      "   [ 0.95294118  0.96862745  0.97647059]\n",
      "   [ 0.95294118  0.96078431  0.95686275]\n",
      "   ..., \n",
      "   [ 0.44313725  0.35686275  0.29803922]\n",
      "   [ 0.47843137  0.4         0.34509804]\n",
      "   [ 0.63137255  0.56862745  0.52941176]]\n",
      "\n",
      "  [[ 0.95686275  0.96078431  0.97647059]\n",
      "   [ 0.96078431  0.97647059  0.98039216]\n",
      "   [ 0.96862745  0.96862745  0.95686275]\n",
      "   ..., \n",
      "   [ 0.52941176  0.42745098  0.36470588]\n",
      "   [ 0.4745098   0.37647059  0.31764706]\n",
      "   [ 0.50588235  0.43137255  0.38823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.71372549  0.6627451   0.63529412]\n",
      "   [ 0.77647059  0.7254902   0.69803922]\n",
      "   [ 0.85490196  0.80392157  0.77647059]\n",
      "   ..., \n",
      "   [ 0.61568627  0.54901961  0.44313725]\n",
      "   [ 0.36862745  0.30588235  0.21960784]\n",
      "   [ 0.50980392  0.45882353  0.41176471]]\n",
      "\n",
      "  [[ 0.54509804  0.4627451   0.41960784]\n",
      "   [ 0.49411765  0.40392157  0.35686275]\n",
      "   [ 0.5254902   0.43529412  0.38431373]\n",
      "   ..., \n",
      "   [ 0.61960784  0.55686275  0.43921569]\n",
      "   [ 0.46666667  0.40784314  0.31372549]\n",
      "   [ 0.45490196  0.40784314  0.35294118]]\n",
      "\n",
      "  [[ 0.76862745  0.70196078  0.62745098]\n",
      "   [ 0.7254902   0.63921569  0.54509804]\n",
      "   [ 0.69019608  0.58823529  0.48235294]\n",
      "   ..., \n",
      "   [ 0.59607843  0.54901961  0.42352941]\n",
      "   [ 0.69411765  0.64313725  0.5372549 ]\n",
      "   [ 0.63529412  0.59607843  0.52156863]]]\n",
      "\n",
      "\n",
      " [[[ 0.74117647  0.89803922  0.94117647]\n",
      "   [ 0.76470588  0.90980392  0.94901961]\n",
      "   [ 0.79607843  0.93333333  0.96470588]\n",
      "   ..., \n",
      "   [ 0.65882353  0.75686275  0.81176471]\n",
      "   [ 0.65882353  0.74901961  0.79215686]\n",
      "   [ 0.65882353  0.7372549   0.78039216]]\n",
      "\n",
      "  [[ 0.79607843  0.9372549   0.96470588]\n",
      "   [ 0.83137255  0.96470588  0.98431373]\n",
      "   [ 0.82352941  0.94901961  0.96862745]\n",
      "   ..., \n",
      "   [ 0.58431373  0.67843137  0.76470588]\n",
      "   [ 0.59215686  0.6745098   0.76470588]\n",
      "   [ 0.60784314  0.67843137  0.77254902]]\n",
      "\n",
      "  [[ 0.83529412  0.96862745  0.98431373]\n",
      "   [ 0.81568627  0.94509804  0.96078431]\n",
      "   [ 0.82745098  0.94509804  0.95686275]\n",
      "   ..., \n",
      "   [ 0.58431373  0.6745098   0.76862745]\n",
      "   [ 0.57647059  0.65490196  0.76470588]\n",
      "   [ 0.56470588  0.63137255  0.75686275]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.31764706  0.35294118]\n",
      "   [ 0.21176471  0.32156863  0.47843137]\n",
      "   [ 0.15294118  0.40784314  0.62352941]\n",
      "   ..., \n",
      "   [ 0.18039216  0.15294118  0.18039216]\n",
      "   [ 0.19607843  0.16862745  0.2       ]\n",
      "   [ 0.27058824  0.24705882  0.25098039]]\n",
      "\n",
      "  [[ 0.25490196  0.2627451   0.28627451]\n",
      "   [ 0.16470588  0.22745098  0.35686275]\n",
      "   [ 0.17647059  0.36078431  0.50980392]\n",
      "   ..., \n",
      "   [ 0.14509804  0.08627451  0.17647059]\n",
      "   [ 0.17254902  0.10588235  0.18431373]\n",
      "   [ 0.22352941  0.17254902  0.2       ]]\n",
      "\n",
      "  [[ 0.20784314  0.22745098  0.24705882]\n",
      "   [ 0.15294118  0.2         0.25490196]\n",
      "   [ 0.24705882  0.30588235  0.36862745]\n",
      "   ..., \n",
      "   [ 0.18039216  0.10196078  0.19215686]\n",
      "   [ 0.23137255  0.14901961  0.18431373]\n",
      "   [ 0.29411765  0.23921569  0.2       ]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  1, CIFAR-10 Batch 3:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.00392157  0.00392157]\n",
      "   ..., \n",
      "   [ 0.03137255  0.03137255  0.01176471]\n",
      "   [ 0.03529412  0.03529412  0.01568627]\n",
      "   [ 0.03921569  0.02745098  0.01176471]]\n",
      "\n",
      "  [[ 0.00392157  0.00392157  0.00392157]\n",
      "   [ 0.00784314  0.00392157  0.00392157]\n",
      "   [ 0.00784314  0.00784314  0.00392157]\n",
      "   ..., \n",
      "   [ 0.02745098  0.02745098  0.01176471]\n",
      "   [ 0.03921569  0.03529412  0.01568627]\n",
      "   [ 0.03921569  0.03137255  0.01568627]]\n",
      "\n",
      "  [[ 0.02745098  0.01176471  0.00784314]\n",
      "   [ 0.02352941  0.01176471  0.00392157]\n",
      "   [ 0.03137255  0.01568627  0.00784314]\n",
      "   ..., \n",
      "   [ 0.03137255  0.02745098  0.01568627]\n",
      "   [ 0.04313725  0.03921569  0.01960784]\n",
      "   [ 0.03921569  0.03529412  0.01960784]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.32941176  0.2627451   0.19215686]\n",
      "   [ 0.35686275  0.29411765  0.21960784]\n",
      "   [ 0.37647059  0.31372549  0.23529412]\n",
      "   ..., \n",
      "   [ 0.49411765  0.41568627  0.3254902 ]\n",
      "   [ 0.50588235  0.42352941  0.3372549 ]\n",
      "   [ 0.48627451  0.40784314  0.32941176]]\n",
      "\n",
      "  [[ 0.32156863  0.26666667  0.2       ]\n",
      "   [ 0.34901961  0.29411765  0.21960784]\n",
      "   [ 0.36470588  0.30980392  0.23529412]\n",
      "   ..., \n",
      "   [ 0.48627451  0.40784314  0.32156863]\n",
      "   [ 0.50196078  0.41960784  0.3372549 ]\n",
      "   [ 0.49019608  0.41960784  0.3372549 ]]\n",
      "\n",
      "  [[ 0.30588235  0.2627451   0.19607843]\n",
      "   [ 0.3372549   0.29019608  0.21960784]\n",
      "   [ 0.35294118  0.30588235  0.23137255]\n",
      "   ..., \n",
      "   [ 0.48235294  0.41176471  0.31764706]\n",
      "   [ 0.49411765  0.42352941  0.33333333]\n",
      "   [ 0.48627451  0.41176471  0.33333333]]]\n",
      "\n",
      "\n",
      " [[[ 0.37647059  0.34117647  0.29803922]\n",
      "   [ 0.41960784  0.38431373  0.34509804]\n",
      "   [ 0.45490196  0.41176471  0.37647059]\n",
      "   ..., \n",
      "   [ 0.8627451   0.76470588  0.68627451]\n",
      "   [ 0.85882353  0.75686275  0.67843137]\n",
      "   [ 0.81960784  0.74117647  0.66666667]]\n",
      "\n",
      "  [[ 0.39607843  0.35686275  0.32156863]\n",
      "   [ 0.43529412  0.4         0.36470588]\n",
      "   [ 0.46666667  0.43137255  0.38823529]\n",
      "   ..., \n",
      "   [ 0.84705882  0.77254902  0.69411765]\n",
      "   [ 0.85098039  0.77647059  0.69803922]\n",
      "   [ 0.84313725  0.75294118  0.6627451 ]]\n",
      "\n",
      "  [[ 0.43529412  0.39215686  0.35686275]\n",
      "   [ 0.48235294  0.43529412  0.4       ]\n",
      "   [ 0.51764706  0.47058824  0.43529412]\n",
      "   ..., \n",
      "   [ 0.87843137  0.8         0.71764706]\n",
      "   [ 0.88627451  0.80784314  0.72156863]\n",
      "   [ 0.87058824  0.78431373  0.69411765]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.55686275  0.62352941  0.62352941]\n",
      "   [ 0.56862745  0.63529412  0.63921569]\n",
      "   [ 0.54509804  0.61176471  0.61176471]\n",
      "   ..., \n",
      "   [ 0.71764706  0.6627451   0.65882353]\n",
      "   [ 0.69411765  0.61176471  0.56862745]\n",
      "   [ 0.6745098   0.60392157  0.55294118]]\n",
      "\n",
      "  [[ 0.56078431  0.63137255  0.62745098]\n",
      "   [ 0.56862745  0.63921569  0.63921569]\n",
      "   [ 0.55686275  0.62352941  0.62745098]\n",
      "   ..., \n",
      "   [ 0.76470588  0.70980392  0.70196078]\n",
      "   [ 0.75294118  0.65490196  0.6       ]\n",
      "   [ 0.75294118  0.66666667  0.60784314]]\n",
      "\n",
      "  [[ 0.54901961  0.62352941  0.61960784]\n",
      "   [ 0.56470588  0.64313725  0.64313725]\n",
      "   [ 0.55686275  0.62745098  0.63137255]\n",
      "   ..., \n",
      "   [ 0.80392157  0.81568627  0.85098039]\n",
      "   [ 0.83137255  0.78039216  0.75294118]\n",
      "   [ 0.81568627  0.76078431  0.70980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.6745098   0.67843137  0.59607843]\n",
      "   [ 0.67058824  0.6745098   0.59607843]\n",
      "   [ 0.69803922  0.69803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.41960784  0.41176471  0.30196078]\n",
      "   [ 0.41568627  0.40392157  0.30980392]\n",
      "   [ 0.4         0.38823529  0.30588235]]\n",
      "\n",
      "  [[ 0.64705882  0.63921569  0.56078431]\n",
      "   [ 0.62352941  0.61568627  0.54117647]\n",
      "   [ 0.70588235  0.69803922  0.62352941]\n",
      "   ..., \n",
      "   [ 0.45882353  0.43921569  0.3254902 ]\n",
      "   [ 0.45882353  0.43921569  0.3372549 ]\n",
      "   [ 0.43529412  0.41568627  0.3254902 ]]\n",
      "\n",
      "  [[ 0.68235294  0.6627451   0.58823529]\n",
      "   [ 0.61176471  0.59215686  0.51764706]\n",
      "   [ 0.68235294  0.6627451   0.58823529]\n",
      "   ..., \n",
      "   [ 0.47058824  0.44705882  0.3254902 ]\n",
      "   [ 0.4745098   0.44705882  0.3372549 ]\n",
      "   [ 0.46666667  0.43921569  0.34509804]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.47843137  0.45882353  0.36862745]\n",
      "   [ 0.47058824  0.45098039  0.36470588]\n",
      "   [ 0.45490196  0.43921569  0.34901961]\n",
      "   ..., \n",
      "   [ 0.48627451  0.49803922  0.39215686]\n",
      "   [ 0.4745098   0.49411765  0.39215686]\n",
      "   [ 0.45882353  0.47843137  0.38039216]]\n",
      "\n",
      "  [[ 0.43529412  0.41176471  0.30588235]\n",
      "   [ 0.43921569  0.41960784  0.3254902 ]\n",
      "   [ 0.48235294  0.48627451  0.39607843]\n",
      "   ..., \n",
      "   [ 0.47843137  0.45490196  0.32941176]\n",
      "   [ 0.45882353  0.4627451   0.35294118]\n",
      "   [ 0.44313725  0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.43921569  0.41176471  0.30196078]\n",
      "   [ 0.45098039  0.42352941  0.32941176]\n",
      "   [ 0.4627451   0.47058824  0.37647059]\n",
      "   ..., \n",
      "   [ 0.48627451  0.45098039  0.31372549]\n",
      "   [ 0.43137255  0.43137255  0.31764706]\n",
      "   [ 0.4         0.41568627  0.30980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.37254902  0.34509804  0.2       ]\n",
      "   [ 0.36078431  0.34509804  0.19215686]\n",
      "   [ 0.35686275  0.34117647  0.19607843]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20392157  0.11764706]\n",
      "   [ 0.20392157  0.2         0.11764706]\n",
      "   [ 0.21176471  0.21176471  0.1254902 ]]\n",
      "\n",
      "  [[ 0.4         0.36862745  0.20784314]\n",
      "   [ 0.41568627  0.38039216  0.2       ]\n",
      "   [ 0.41960784  0.38431373  0.21176471]\n",
      "   ..., \n",
      "   [ 0.20392157  0.20392157  0.1254902 ]\n",
      "   [ 0.19215686  0.18823529  0.10588235]\n",
      "   [ 0.23137255  0.22745098  0.1254902 ]]\n",
      "\n",
      "  [[ 0.41960784  0.38431373  0.20392157]\n",
      "   [ 0.42745098  0.38823529  0.20392157]\n",
      "   [ 0.41176471  0.38039216  0.20392157]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20784314  0.1254902 ]\n",
      "   [ 0.21568627  0.20784314  0.11764706]\n",
      "   [ 0.24313725  0.23529412  0.1254902 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.37647059  0.36078431  0.23529412]\n",
      "   [ 0.24313725  0.23921569  0.15686275]\n",
      "   [ 0.20784314  0.21176471  0.14117647]\n",
      "   ..., \n",
      "   [ 0.19607843  0.19607843  0.12941176]\n",
      "   [ 0.21568627  0.21568627  0.14117647]\n",
      "   [ 0.23529412  0.22745098  0.15294118]]\n",
      "\n",
      "  [[ 0.31372549  0.29803922  0.2       ]\n",
      "   [ 0.23137255  0.22745098  0.15294118]\n",
      "   [ 0.19607843  0.2         0.12941176]\n",
      "   ..., \n",
      "   [ 0.21960784  0.20392157  0.13333333]\n",
      "   [ 0.25098039  0.23529412  0.15294118]\n",
      "   [ 0.25098039  0.23529412  0.14901961]]\n",
      "\n",
      "  [[ 0.18431373  0.18039216  0.1254902 ]\n",
      "   [ 0.15294118  0.15686275  0.10980392]\n",
      "   [ 0.15294118  0.15686275  0.09803922]\n",
      "   ..., \n",
      "   [ 0.30980392  0.29019608  0.17647059]\n",
      "   [ 0.25098039  0.23137255  0.14901961]\n",
      "   [ 0.23921569  0.22352941  0.14901961]]]\n",
      "\n",
      "\n",
      " [[[ 0.60784314  0.40392157  0.43137255]\n",
      "   [ 0.59607843  0.39215686  0.41960784]\n",
      "   [ 0.60784314  0.40392157  0.43137255]\n",
      "   ..., \n",
      "   [ 0.23137255  0.14901961  0.17254902]\n",
      "   [ 0.22352941  0.15686275  0.18039216]\n",
      "   [ 0.22352941  0.16078431  0.19607843]]\n",
      "\n",
      "  [[ 0.60784314  0.40784314  0.43529412]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   [ 0.6         0.4         0.42745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.18823529  0.20784314]\n",
      "   [ 0.21960784  0.15294118  0.17647059]\n",
      "   [ 0.21960784  0.16078431  0.19215686]]\n",
      "\n",
      "  [[ 0.61176471  0.41176471  0.43921569]\n",
      "   [ 0.58823529  0.38823529  0.41568627]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   ..., \n",
      "   [ 0.41176471  0.30980392  0.3254902 ]\n",
      "   [ 0.23137255  0.16470588  0.18823529]\n",
      "   [ 0.2         0.14117647  0.17254902]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.45490196  0.43921569  0.5254902 ]\n",
      "   [ 0.44313725  0.43529412  0.50980392]\n",
      "   [ 0.44313725  0.44313725  0.50196078]\n",
      "   ..., \n",
      "   [ 0.42745098  0.38039216  0.44705882]\n",
      "   [ 0.28235294  0.23921569  0.29803922]\n",
      "   [ 0.42352941  0.37647059  0.43529412]]\n",
      "\n",
      "  [[ 0.45490196  0.44313725  0.52156863]\n",
      "   [ 0.44705882  0.43529412  0.50980392]\n",
      "   [ 0.45098039  0.43921569  0.51372549]\n",
      "   ..., \n",
      "   [ 0.43137255  0.41176471  0.48627451]\n",
      "   [ 0.23137255  0.19607843  0.26666667]\n",
      "   [ 0.29411765  0.23921569  0.30980392]]\n",
      "\n",
      "  [[ 0.46666667  0.45490196  0.52941176]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   ..., \n",
      "   [ 0.47058824  0.45882353  0.5372549 ]\n",
      "   [ 0.39607843  0.37254902  0.44705882]\n",
      "   [ 0.24705882  0.19607843  0.26666667]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  1, CIFAR-10 Batch 4:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.58823529  0.54117647  0.51764706]\n",
      "   [ 0.58039216  0.5372549   0.51372549]\n",
      "   [ 0.60392157  0.56078431  0.5372549 ]\n",
      "   ..., \n",
      "   [ 0.60784314  0.56078431  0.52941176]\n",
      "   [ 0.64705882  0.58823529  0.55686275]\n",
      "   [ 0.65882353  0.58823529  0.56078431]]\n",
      "\n",
      "  [[ 0.56862745  0.5254902   0.50196078]\n",
      "   [ 0.54509804  0.50196078  0.47843137]\n",
      "   [ 0.57254902  0.52941176  0.50588235]\n",
      "   ..., \n",
      "   [ 0.60784314  0.56078431  0.52941176]\n",
      "   [ 0.64313725  0.58431373  0.55686275]\n",
      "   [ 0.69019608  0.62352941  0.59215686]]\n",
      "\n",
      "  [[ 0.57254902  0.52941176  0.50588235]\n",
      "   [ 0.56862745  0.5254902   0.50196078]\n",
      "   [ 0.56862745  0.5254902   0.50196078]\n",
      "   ..., \n",
      "   [ 0.61176471  0.56470588  0.53333333]\n",
      "   [ 0.63137255  0.57254902  0.54117647]\n",
      "   [ 0.6745098   0.60784314  0.58039216]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.55294118  0.52941176  0.48235294]\n",
      "   [ 0.49803922  0.4745098   0.42745098]\n",
      "   [ 0.52156863  0.49803922  0.45098039]\n",
      "   ..., \n",
      "   [ 0.42352941  0.42352941  0.38431373]\n",
      "   [ 0.4         0.4         0.36078431]\n",
      "   [ 0.36862745  0.36470588  0.3254902 ]]\n",
      "\n",
      "  [[ 0.49803922  0.4745098   0.42352941]\n",
      "   [ 0.49019608  0.46666667  0.41960784]\n",
      "   [ 0.51764706  0.49411765  0.44705882]\n",
      "   ..., \n",
      "   [ 0.30980392  0.30980392  0.27058824]\n",
      "   [ 0.25490196  0.25490196  0.21568627]\n",
      "   [ 0.26666667  0.26666667  0.22745098]]\n",
      "\n",
      "  [[ 0.54901961  0.5254902   0.47843137]\n",
      "   [ 0.47058824  0.44705882  0.4       ]\n",
      "   [ 0.50196078  0.47843137  0.43137255]\n",
      "   ..., \n",
      "   [ 0.25490196  0.25490196  0.21568627]\n",
      "   [ 0.25882353  0.25882353  0.21960784]\n",
      "   [ 0.2745098   0.2745098   0.23529412]]]\n",
      "\n",
      "\n",
      " [[[ 0.6         0.59607843  0.43137255]\n",
      "   [ 0.61176471  0.60784314  0.44313725]\n",
      "   [ 0.58823529  0.58431373  0.41960784]\n",
      "   ..., \n",
      "   [ 0.60784314  0.59607843  0.41960784]\n",
      "   [ 0.57647059  0.56470588  0.38431373]\n",
      "   [ 0.58039216  0.57647059  0.39607843]]\n",
      "\n",
      "  [[ 0.57254902  0.55686275  0.39607843]\n",
      "   [ 0.60784314  0.59215686  0.44313725]\n",
      "   [ 0.60392157  0.58823529  0.44705882]\n",
      "   ..., \n",
      "   [ 0.64313725  0.63137255  0.45490196]\n",
      "   [ 0.58039216  0.56862745  0.39607843]\n",
      "   [ 0.56862745  0.56078431  0.39215686]]\n",
      "\n",
      "  [[ 0.59215686  0.56470588  0.40784314]\n",
      "   [ 0.60392157  0.57254902  0.43529412]\n",
      "   [ 0.59215686  0.55686275  0.43921569]\n",
      "   ..., \n",
      "   [ 0.63529412  0.62352941  0.44705882]\n",
      "   [ 0.61568627  0.60392157  0.42745098]\n",
      "   [ 0.56862745  0.56078431  0.4       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.70196078  0.70980392  0.61568627]\n",
      "   [ 0.67058824  0.67058824  0.53333333]\n",
      "   [ 0.66666667  0.65490196  0.50196078]\n",
      "   ..., \n",
      "   [ 0.66666667  0.65490196  0.49411765]\n",
      "   [ 0.65882353  0.64313725  0.48627451]\n",
      "   [ 0.65490196  0.62352941  0.47843137]]\n",
      "\n",
      "  [[ 0.70980392  0.70980392  0.61176471]\n",
      "   [ 0.6627451   0.6627451   0.5254902 ]\n",
      "   [ 0.6627451   0.65490196  0.50588235]\n",
      "   ..., \n",
      "   [ 0.6627451   0.65098039  0.49019608]\n",
      "   [ 0.67058824  0.65490196  0.49803922]\n",
      "   [ 0.66666667  0.64705882  0.50196078]]\n",
      "\n",
      "  [[ 0.69803922  0.68627451  0.57647059]\n",
      "   [ 0.66666667  0.65882353  0.52156863]\n",
      "   [ 0.67058824  0.66666667  0.52156863]\n",
      "   ..., \n",
      "   [ 0.65098039  0.63921569  0.47843137]\n",
      "   [ 0.6627451   0.65490196  0.49411765]\n",
      "   [ 0.65490196  0.65490196  0.50980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.27058824  0.34509804  0.44705882]\n",
      "   [ 0.35294118  0.4745098   0.58823529]\n",
      "   [ 0.35294118  0.49803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.00784314  0.01176471  0.07058824]\n",
      "   [ 0.00784314  0.00784314  0.0627451 ]\n",
      "   [ 0.00784314  0.00784314  0.05882353]]\n",
      "\n",
      "  [[ 0.11372549  0.15294118  0.25098039]\n",
      "   [ 0.05882353  0.10588235  0.20784314]\n",
      "   [ 0.05098039  0.09803922  0.2       ]\n",
      "   ..., \n",
      "   [ 0.00392157  0.          0.00784314]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.00392157  0.00784314]]\n",
      "\n",
      "  [[ 0.01568627  0.01176471  0.01960784]\n",
      "   [ 0.01568627  0.00392157  0.00784314]\n",
      "   [ 0.01176471  0.00392157  0.00784314]\n",
      "   ..., \n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.00784314]\n",
      "   [ 0.          0.          0.01176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.00392157  0.00392157  0.01960784]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.        ]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03137255  0.04313725]\n",
      "   [ 0.00784314  0.00784314  0.01568627]\n",
      "   [ 0.00392157  0.00392157  0.01568627]]\n",
      "\n",
      "  [[ 0.03137255  0.0627451   0.12941176]\n",
      "   [ 0.00392157  0.01568627  0.05098039]\n",
      "   [ 0.          0.00392157  0.01176471]\n",
      "   ..., \n",
      "   [ 0.02745098  0.03921569  0.09019608]\n",
      "   [ 0.01176471  0.01568627  0.03921569]\n",
      "   [ 0.00392157  0.00392157  0.01176471]]\n",
      "\n",
      "  [[ 0.14509804  0.25098039  0.42352941]\n",
      "   [ 0.09411765  0.16862745  0.30196078]\n",
      "   [ 0.03921569  0.0745098   0.15294118]\n",
      "   ..., \n",
      "   [ 0.07843137  0.11764706  0.23921569]\n",
      "   [ 0.05098039  0.07843137  0.16862745]\n",
      "   [ 0.02352941  0.03921569  0.09019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.7254902   0.79215686  0.81176471]\n",
      "   [ 0.67843137  0.77647059  0.80392157]\n",
      "   [ 0.69019608  0.81176471  0.85098039]\n",
      "   ..., \n",
      "   [ 0.14509804  0.17647059  0.24313725]\n",
      "   [ 0.10196078  0.1372549   0.2       ]\n",
      "   [ 0.12941176  0.19607843  0.27058824]]\n",
      "\n",
      "  [[ 0.42745098  0.47058824  0.46666667]\n",
      "   [ 0.4627451   0.52941176  0.52941176]\n",
      "   [ 0.4745098   0.56078431  0.56470588]\n",
      "   ..., \n",
      "   [ 0.16862745  0.19607843  0.23529412]\n",
      "   [ 0.12941176  0.13333333  0.16862745]\n",
      "   [ 0.15686275  0.18823529  0.22745098]]\n",
      "\n",
      "  [[ 0.20392157  0.22745098  0.21568627]\n",
      "   [ 0.22745098  0.26666667  0.25098039]\n",
      "   [ 0.22745098  0.28235294  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15686275  0.18431373  0.22745098]\n",
      "   [ 0.18431373  0.22745098  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.63529412  0.62745098  0.6745098 ]\n",
      "   [ 0.61960784  0.61176471  0.65490196]\n",
      "   [ 0.62352941  0.61568627  0.65882353]\n",
      "   ..., \n",
      "   [ 0.05882353  0.05098039  0.09019608]\n",
      "   [ 0.05490196  0.04705882  0.09019608]\n",
      "   [ 0.07058824  0.0627451   0.10588235]]\n",
      "\n",
      "  [[ 0.61176471  0.60392157  0.65882353]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   ..., \n",
      "   [ 0.05098039  0.04313725  0.08235294]\n",
      "   [ 0.05882353  0.05098039  0.09411765]\n",
      "   [ 0.1254902   0.11764706  0.16078431]]\n",
      "\n",
      "  [[ 0.58431373  0.57647059  0.63137255]\n",
      "   [ 0.56470588  0.55686275  0.61176471]\n",
      "   [ 0.57254902  0.56470588  0.61960784]\n",
      "   ..., \n",
      "   [ 0.0627451   0.05490196  0.09411765]\n",
      "   [ 0.20392157  0.19607843  0.23921569]\n",
      "   [ 0.38431373  0.37647059  0.41960784]]]\n",
      "\n",
      "\n",
      " [[[ 0.76470588  0.71764706  0.67058824]\n",
      "   [ 0.75686275  0.70980392  0.6627451 ]\n",
      "   [ 0.76078431  0.71372549  0.66666667]\n",
      "   ..., \n",
      "   [ 0.22352941  0.22352941  0.22352941]\n",
      "   [ 0.20392157  0.20392157  0.20392157]\n",
      "   [ 0.03137255  0.03137255  0.03137255]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.67843137]\n",
      "   [ 0.76470588  0.71764706  0.6745098 ]\n",
      "   [ 0.77254902  0.7254902   0.68235294]\n",
      "   ..., \n",
      "   [ 0.34117647  0.34117647  0.34117647]\n",
      "   [ 0.31372549  0.31372549  0.31372549]\n",
      "   [ 0.03921569  0.03921569  0.03921569]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.68627451]\n",
      "   [ 0.76862745  0.71764706  0.68235294]\n",
      "   [ 0.77647059  0.7254902   0.69411765]\n",
      "   ..., \n",
      "   [ 0.43137255  0.43137255  0.43137255]\n",
      "   [ 0.41568627  0.41568627  0.41568627]\n",
      "   [ 0.04705882  0.04705882  0.04705882]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.78039216  0.7254902   0.69411765]\n",
      "   [ 0.76862745  0.72156863  0.68627451]\n",
      "   [ 0.78039216  0.74117647  0.69803922]\n",
      "   ..., \n",
      "   [ 0.13333333  0.10980392  0.08235294]\n",
      "   [ 0.10980392  0.09019608  0.07058824]\n",
      "   [ 0.08627451  0.0745098   0.0627451 ]]\n",
      "\n",
      "  [[ 0.77254902  0.7254902   0.69019608]\n",
      "   [ 0.76470588  0.71764706  0.68235294]\n",
      "   [ 0.77254902  0.72941176  0.69411765]\n",
      "   ..., \n",
      "   [ 0.15294118  0.12156863  0.08235294]\n",
      "   [ 0.14509804  0.12156863  0.08627451]\n",
      "   [ 0.1372549   0.11372549  0.08627451]]\n",
      "\n",
      "  [[ 0.75686275  0.71764706  0.68235294]\n",
      "   [ 0.75686275  0.70588235  0.6745098 ]\n",
      "   [ 0.76470588  0.70980392  0.67843137]\n",
      "   ..., \n",
      "   [ 0.16470588  0.12941176  0.08235294]\n",
      "   [ 0.16862745  0.12941176  0.09019608]\n",
      "   [ 0.16078431  0.1254902   0.09411765]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  1, CIFAR-10 Batch 5:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.05490196  0.06666667  0.11372549]\n",
      "   [ 0.05098039  0.05490196  0.10196078]\n",
      "   [ 0.04705882  0.07843137  0.11372549]\n",
      "   ..., \n",
      "   [ 0.09019608  0.19215686  0.17647059]\n",
      "   [ 0.08627451  0.17647059  0.14901961]\n",
      "   [ 0.07843137  0.16078431  0.13333333]]\n",
      "\n",
      "  [[ 0.09411765  0.11372549  0.16078431]\n",
      "   [ 0.09803922  0.14117647  0.1254902 ]\n",
      "   [ 0.06666667  0.1254902   0.10588235]\n",
      "   ..., \n",
      "   [ 0.0627451   0.16862745  0.13333333]\n",
      "   [ 0.10196078  0.20392157  0.15686275]\n",
      "   [ 0.09019608  0.18823529  0.14117647]]\n",
      "\n",
      "  [[ 0.13333333  0.16470588  0.19607843]\n",
      "   [ 0.10588235  0.17647059  0.1372549 ]\n",
      "   [ 0.10588235  0.18823529  0.14117647]\n",
      "   ..., \n",
      "   [ 0.09803922  0.21176471  0.16078431]\n",
      "   [ 0.10588235  0.22745098  0.15686275]\n",
      "   [ 0.12156863  0.23529412  0.16862745]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.79607843  0.78823529  0.78039216]\n",
      "   [ 0.78823529  0.78431373  0.76470588]\n",
      "   [ 0.79607843  0.78039216  0.78039216]\n",
      "   ..., \n",
      "   [ 0.3254902   0.3254902   0.22352941]\n",
      "   [ 0.34509804  0.38431373  0.29411765]\n",
      "   [ 0.35686275  0.35686275  0.28627451]]\n",
      "\n",
      "  [[ 0.81568627  0.8         0.78823529]\n",
      "   [ 0.81568627  0.8         0.78823529]\n",
      "   [ 0.81960784  0.80392157  0.79215686]\n",
      "   ..., \n",
      "   [ 0.40392157  0.38431373  0.28235294]\n",
      "   [ 0.25882353  0.30196078  0.21176471]\n",
      "   [ 0.25882353  0.28627451  0.21960784]]\n",
      "\n",
      "  [[ 0.85882353  0.84313725  0.82745098]\n",
      "   [ 0.85098039  0.83529412  0.82352941]\n",
      "   [ 0.85490196  0.83921569  0.82745098]\n",
      "   ..., \n",
      "   [ 0.42352941  0.40392157  0.31764706]\n",
      "   [ 0.41960784  0.41176471  0.3372549 ]\n",
      "   [ 0.29803922  0.33333333  0.25882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.56862745  0.54117647  0.44313725]\n",
      "   [ 0.70980392  0.68627451  0.63529412]\n",
      "   [ 0.72156863  0.71372549  0.6627451 ]\n",
      "   ..., \n",
      "   [ 0.78823529  0.79215686  0.82745098]\n",
      "   [ 0.77647059  0.78431373  0.81960784]\n",
      "   [ 0.78823529  0.79607843  0.83137255]]\n",
      "\n",
      "  [[ 0.43921569  0.39607843  0.20784314]\n",
      "   [ 0.55686275  0.51764706  0.34509804]\n",
      "   [ 0.54509804  0.51764706  0.31764706]\n",
      "   ..., \n",
      "   [ 0.74901961  0.77647059  0.81960784]\n",
      "   [ 0.74117647  0.77254902  0.81568627]\n",
      "   [ 0.76078431  0.79215686  0.83921569]]\n",
      "\n",
      "  [[ 0.40784314  0.34117647  0.14901961]\n",
      "   [ 0.40784314  0.34117647  0.14117647]\n",
      "   [ 0.38431373  0.3254902   0.07843137]\n",
      "   ..., \n",
      "   [ 0.74901961  0.79215686  0.83921569]\n",
      "   [ 0.75686275  0.8         0.84705882]\n",
      "   [ 0.77254902  0.81568627  0.8627451 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.6627451   0.49019608  0.34901961]\n",
      "   [ 0.63921569  0.46666667  0.32941176]\n",
      "   [ 0.63137255  0.45098039  0.31764706]\n",
      "   ..., \n",
      "   [ 0.19215686  0.16862745  0.16470588]\n",
      "   [ 0.16862745  0.14509804  0.14509804]\n",
      "   [ 0.16078431  0.13333333  0.13333333]]\n",
      "\n",
      "  [[ 0.72941176  0.56862745  0.39607843]\n",
      "   [ 0.7372549   0.55686275  0.38431373]\n",
      "   [ 0.70980392  0.53333333  0.38039216]\n",
      "   ..., \n",
      "   [ 0.21960784  0.19607843  0.2       ]\n",
      "   [ 0.19607843  0.17647059  0.18039216]\n",
      "   [ 0.18431373  0.16470588  0.16862745]]\n",
      "\n",
      "  [[ 0.67843137  0.5372549   0.40392157]\n",
      "   [ 0.74117647  0.54901961  0.38431373]\n",
      "   [ 0.65098039  0.49411765  0.35686275]\n",
      "   ..., \n",
      "   [ 0.17254902  0.16078431  0.16862745]\n",
      "   [ 0.15686275  0.14509804  0.15686275]\n",
      "   [ 0.14509804  0.1372549   0.14509804]]]\n",
      "\n",
      "\n",
      " [[[ 0.23921569  0.28627451  0.29803922]\n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15294118  0.19215686  0.25882353]\n",
      "   ..., \n",
      "   [ 0.27843137  0.35294118  0.31372549]\n",
      "   [ 0.24313725  0.30588235  0.29411765]\n",
      "   [ 0.17647059  0.22745098  0.23921569]]\n",
      "\n",
      "  [[ 0.24705882  0.29411765  0.30196078]\n",
      "   [ 0.17647059  0.22745098  0.2627451 ]\n",
      "   [ 0.1254902   0.16862745  0.22745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.34901961  0.32156863]\n",
      "   [ 0.27843137  0.32941176  0.31372549]\n",
      "   [ 0.19607843  0.23529412  0.24313725]]\n",
      "\n",
      "  [[ 0.24705882  0.31372549  0.30196078]\n",
      "   [ 0.22352941  0.29411765  0.29411765]\n",
      "   [ 0.24705882  0.30980392  0.31764706]\n",
      "   ..., \n",
      "   [ 0.32156863  0.37647059  0.35686275]\n",
      "   [ 0.29803922  0.34509804  0.32156863]\n",
      "   [ 0.20784314  0.24705882  0.24313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.51372549  0.52156863  0.4       ]\n",
      "   [ 0.61176471  0.60392157  0.4627451 ]\n",
      "   [ 0.63137255  0.61568627  0.4745098 ]\n",
      "   ..., \n",
      "   [ 0.91764706  0.8745098   0.67843137]\n",
      "   [ 0.8627451   0.8         0.61176471]\n",
      "   [ 0.70980392  0.65098039  0.49411765]]\n",
      "\n",
      "  [[ 0.41176471  0.42352941  0.3254902 ]\n",
      "   [ 0.41960784  0.42352941  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.32941176]\n",
      "   ..., \n",
      "   [ 0.92156863  0.86666667  0.6745098 ]\n",
      "   [ 0.89803922  0.81568627  0.63137255]\n",
      "   [ 0.74901961  0.66666667  0.51372549]]\n",
      "\n",
      "  [[ 0.2627451   0.29019608  0.23921569]\n",
      "   [ 0.30588235  0.3254902   0.26666667]\n",
      "   [ 0.43921569  0.45098039  0.35294118]\n",
      "   ..., \n",
      "   [ 0.89019608  0.80784314  0.58823529]\n",
      "   [ 0.85098039  0.75294118  0.54509804]\n",
      "   [ 0.72156863  0.62745098  0.45882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.02352941  0.05882353]\n",
      "   [ 0.07843137  0.08627451  0.09019608]\n",
      "   ..., \n",
      "   [ 0.23137255  0.27843137  0.21568627]\n",
      "   [ 0.22352941  0.2745098   0.21568627]\n",
      "   [ 0.20784314  0.26666667  0.23137255]]\n",
      "\n",
      "  [[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.03529412  0.05882353]\n",
      "   [ 0.09803922  0.1254902   0.10980392]\n",
      "   ..., \n",
      "   [ 0.21568627  0.23921569  0.18823529]\n",
      "   [ 0.25882353  0.29803922  0.22745098]\n",
      "   [ 0.18823529  0.24705882  0.21176471]]\n",
      "\n",
      "  [[ 0.04705882  0.02352941  0.0627451 ]\n",
      "   [ 0.04313725  0.03921569  0.05882353]\n",
      "   [ 0.14901961  0.18431373  0.14901961]\n",
      "   ..., \n",
      "   [ 0.18431373  0.20392157  0.16470588]\n",
      "   [ 0.22352941  0.25882353  0.19215686]\n",
      "   [ 0.20392157  0.25098039  0.2       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.69411765  0.6627451   0.71764706]\n",
      "   [ 0.70588235  0.6627451   0.72156863]\n",
      "   [ 0.72156863  0.6745098   0.74509804]\n",
      "   ..., \n",
      "   [ 0.6745098   0.62352941  0.68235294]\n",
      "   [ 0.66666667  0.61568627  0.67058824]\n",
      "   [ 0.64313725  0.58823529  0.64705882]]\n",
      "\n",
      "  [[ 0.62352941  0.61568627  0.69019608]\n",
      "   [ 0.63529412  0.61568627  0.69411765]\n",
      "   [ 0.65490196  0.63529412  0.71764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.69803922  0.78431373]\n",
      "   [ 0.70980392  0.68627451  0.77254902]\n",
      "   [ 0.69803922  0.67843137  0.76470588]]\n",
      "\n",
      "  [[ 0.61960784  0.61960784  0.72941176]\n",
      "   [ 0.62352941  0.62352941  0.73333333]\n",
      "   [ 0.63921569  0.63921569  0.74509804]\n",
      "   ..., \n",
      "   [ 0.70980392  0.70588235  0.82745098]\n",
      "   [ 0.70196078  0.69411765  0.81568627]\n",
      "   [ 0.68627451  0.68235294  0.80392157]]]\n",
      "\n",
      "\n",
      " [[[ 0.68627451  0.75686275  0.89803922]\n",
      "   [ 0.6745098   0.75294118  0.9254902 ]\n",
      "   [ 0.67058824  0.75686275  0.94117647]\n",
      "   ..., \n",
      "   [ 0.75686275  0.80784314  0.93333333]\n",
      "   [ 0.76862745  0.80784314  0.90588235]\n",
      "   [ 0.76078431  0.79607843  0.89019608]]\n",
      "\n",
      "  [[ 0.65490196  0.73333333  0.88627451]\n",
      "   [ 0.64313725  0.73333333  0.90196078]\n",
      "   [ 0.64313725  0.7372549   0.90980392]\n",
      "   ..., \n",
      "   [ 0.70196078  0.75686275  0.88235294]\n",
      "   [ 0.69803922  0.74901961  0.85098039]\n",
      "   [ 0.69019608  0.73333333  0.83137255]]\n",
      "\n",
      "  [[ 0.65490196  0.72156863  0.86666667]\n",
      "   [ 0.65490196  0.72941176  0.87058824]\n",
      "   [ 0.67058824  0.72941176  0.85098039]\n",
      "   ..., \n",
      "   [ 0.69019608  0.74901961  0.87843137]\n",
      "   [ 0.68235294  0.74117647  0.85098039]\n",
      "   [ 0.67058824  0.72156863  0.82745098]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.33333333  0.32941176  0.39607843]\n",
      "   [ 0.33333333  0.32156863  0.36470588]\n",
      "   [ 0.36078431  0.32941176  0.32156863]\n",
      "   ..., \n",
      "   [ 0.4745098   0.44313725  0.47058824]\n",
      "   [ 0.41960784  0.40392157  0.46666667]\n",
      "   [ 0.45882353  0.43921569  0.50588235]]\n",
      "\n",
      "  [[ 0.33333333  0.34117647  0.4       ]\n",
      "   [ 0.32941176  0.31764706  0.34901961]\n",
      "   [ 0.34117647  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.30196078  0.28235294  0.32941176]\n",
      "   [ 0.43137255  0.40392157  0.47058824]\n",
      "   [ 0.44705882  0.41960784  0.48627451]]\n",
      "\n",
      "  [[ 0.32156863  0.32941176  0.37647059]\n",
      "   [ 0.29411765  0.29019608  0.32156863]\n",
      "   [ 0.22352941  0.19607843  0.21568627]\n",
      "   ..., \n",
      "   [ 0.30196078  0.26666667  0.30588235]\n",
      "   [ 0.35686275  0.30588235  0.35294118]\n",
      "   [ 0.35686275  0.30588235  0.35294118]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  2, CIFAR-10 Batch 1:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.88235294  0.93333333  1.        ]\n",
      "   [ 0.90980392  0.94509804  1.        ]\n",
      "   [ 0.9372549   0.96470588  1.        ]\n",
      "   ..., \n",
      "   [ 0.79607843  0.8         0.83137255]\n",
      "   [ 0.8         0.83137255  0.84705882]\n",
      "   [ 0.69411765  0.7254902   0.73333333]]\n",
      "\n",
      "  [[ 0.82352941  0.87843137  0.96078431]\n",
      "   [ 0.88627451  0.92156863  0.98823529]\n",
      "   [ 0.92156863  0.94509804  1.        ]\n",
      "   ..., \n",
      "   [ 0.63921569  0.64313725  0.64313725]\n",
      "   [ 0.58431373  0.6         0.57647059]\n",
      "   [ 0.60392157  0.62352941  0.58823529]]\n",
      "\n",
      "  [[ 0.76470588  0.78039216  0.83921569]\n",
      "   [ 0.81568627  0.81568627  0.85490196]\n",
      "   [ 0.8627451   0.83921569  0.87843137]\n",
      "   ..., \n",
      "   [ 0.55686275  0.56078431  0.5254902 ]\n",
      "   [ 0.45882353  0.46666667  0.4       ]\n",
      "   [ 0.45490196  0.4627451   0.38823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.63921569  0.63529412  0.60392157]\n",
      "   [ 0.64313725  0.61568627  0.59215686]\n",
      "   [ 0.6627451   0.62745098  0.6       ]\n",
      "   ..., \n",
      "   [ 0.67843137  0.65882353  0.61568627]\n",
      "   [ 0.6627451   0.65490196  0.60784314]\n",
      "   [ 0.64705882  0.63921569  0.59215686]]\n",
      "\n",
      "  [[ 0.61176471  0.60392157  0.56470588]\n",
      "   [ 0.64705882  0.61960784  0.6       ]\n",
      "   [ 0.6745098   0.64313725  0.61568627]\n",
      "   ..., \n",
      "   [ 0.64705882  0.63137255  0.58431373]\n",
      "   [ 0.62745098  0.61960784  0.57254902]\n",
      "   [ 0.62745098  0.61960784  0.56862745]]\n",
      "\n",
      "  [[ 0.62745098  0.6         0.56078431]\n",
      "   [ 0.65490196  0.62745098  0.58823529]\n",
      "   [ 0.65490196  0.62745098  0.59215686]\n",
      "   ..., \n",
      "   [ 0.63921569  0.63137255  0.58431373]\n",
      "   [ 0.63529412  0.62745098  0.58039216]\n",
      "   [ 0.63137255  0.62352941  0.57647059]]]\n",
      "\n",
      "\n",
      " [[[ 0.68235294  0.58823529  0.52941176]\n",
      "   [ 0.72156863  0.56470588  0.43529412]\n",
      "   [ 0.74509804  0.62745098  0.50196078]\n",
      "   ..., \n",
      "   [ 0.71764706  0.63137255  0.5372549 ]\n",
      "   [ 0.70588235  0.59607843  0.49019608]\n",
      "   [ 0.69019608  0.59607843  0.49803922]]\n",
      "\n",
      "  [[ 0.65490196  0.58431373  0.56862745]\n",
      "   [ 0.69019608  0.54901961  0.44705882]\n",
      "   [ 0.74117647  0.63137255  0.52156863]\n",
      "   ..., \n",
      "   [ 0.69019608  0.61568627  0.52941176]\n",
      "   [ 0.67843137  0.58039216  0.48235294]\n",
      "   [ 0.6745098   0.59215686  0.50196078]]\n",
      "\n",
      "  [[ 0.69019608  0.61568627  0.6       ]\n",
      "   [ 0.66666667  0.52941176  0.42352941]\n",
      "   [ 0.73333333  0.63137255  0.51372549]\n",
      "   ..., \n",
      "   [ 0.70196078  0.62352941  0.53333333]\n",
      "   [ 0.69411765  0.59215686  0.49019608]\n",
      "   [ 0.69803922  0.61176471  0.51764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.26666667  0.21960784  0.25882353]\n",
      "   [ 0.23137255  0.18039216  0.21960784]\n",
      "   [ 0.41176471  0.40784314  0.43529412]\n",
      "   ..., \n",
      "   [ 0.69803922  0.61568627  0.54509804]\n",
      "   [ 0.68235294  0.6         0.52156863]\n",
      "   [ 0.6627451   0.59215686  0.5254902 ]]\n",
      "\n",
      "  [[ 0.34509804  0.28627451  0.32156863]\n",
      "   [ 0.35294118  0.31764706  0.34509804]\n",
      "   [ 0.51764706  0.54509804  0.55294118]\n",
      "   ..., \n",
      "   [ 0.68627451  0.61568627  0.55686275]\n",
      "   [ 0.68235294  0.61176471  0.54509804]\n",
      "   [ 0.67058824  0.60392157  0.52941176]]\n",
      "\n",
      "  [[ 0.42745098  0.32941176  0.36862745]\n",
      "   [ 0.40784314  0.34509804  0.37254902]\n",
      "   [ 0.56862745  0.58039216  0.58431373]\n",
      "   ..., \n",
      "   [ 0.6745098   0.60392157  0.54509804]\n",
      "   [ 0.67058824  0.60392157  0.5372549 ]\n",
      "   [ 0.67843137  0.61176471  0.5372549 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.27058824  0.27843137  0.20392157]\n",
      "   [ 0.24313725  0.24313725  0.19215686]\n",
      "   [ 0.22745098  0.22352941  0.18823529]\n",
      "   ..., \n",
      "   [ 0.4627451   0.49019608  0.31372549]\n",
      "   [ 0.4745098   0.49019608  0.28627451]\n",
      "   [ 0.48235294  0.50588235  0.29019608]]\n",
      "\n",
      "  [[ 0.2745098   0.27843137  0.19215686]\n",
      "   [ 0.23137255  0.22745098  0.18431373]\n",
      "   [ 0.2         0.19215686  0.16862745]\n",
      "   ..., \n",
      "   [ 0.48235294  0.48235294  0.3254902 ]\n",
      "   [ 0.45882353  0.47843137  0.29803922]\n",
      "   [ 0.41568627  0.43921569  0.25490196]]\n",
      "\n",
      "  [[ 0.28627451  0.28235294  0.18431373]\n",
      "   [ 0.25490196  0.24705882  0.17647059]\n",
      "   [ 0.20392157  0.19607843  0.17254902]\n",
      "   ..., \n",
      "   [ 0.47058824  0.46666667  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.27058824]\n",
      "   [ 0.42745098  0.43529412  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.76862745  0.82352941  0.37254902]\n",
      "   [ 0.79215686  0.82745098  0.42745098]\n",
      "   [ 0.76078431  0.78823529  0.41176471]\n",
      "   ..., \n",
      "   [ 0.77647059  0.84313725  0.35294118]\n",
      "   [ 0.81568627  0.85882353  0.42352941]\n",
      "   [ 0.83137255  0.88235294  0.43529412]]\n",
      "\n",
      "  [[ 0.74509804  0.80784314  0.36862745]\n",
      "   [ 0.74901961  0.80784314  0.37254902]\n",
      "   [ 0.74509804  0.80392157  0.36470588]\n",
      "   ..., \n",
      "   [ 0.77647059  0.83921569  0.36862745]\n",
      "   [ 0.76470588  0.81568627  0.37254902]\n",
      "   [ 0.80784314  0.8627451   0.41960784]]\n",
      "\n",
      "  [[ 0.71764706  0.76470588  0.36470588]\n",
      "   [ 0.72156863  0.77254902  0.37647059]\n",
      "   [ 0.71372549  0.75686275  0.36078431]\n",
      "   ..., \n",
      "   [ 0.75686275  0.81176471  0.35686275]\n",
      "   [ 0.74117647  0.8         0.34117647]\n",
      "   [ 0.77647059  0.82745098  0.39215686]]]\n",
      "\n",
      "\n",
      " [[[ 0.61960784  0.61960784  0.61960784]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61960784  0.62352941  0.60392157]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59215686]]\n",
      "\n",
      "  [[ 0.61568627  0.61568627  0.61568627]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.6       ]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59607843]]\n",
      "\n",
      "  [[ 0.61176471  0.61176471  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.22745098  0.26666667  0.26666667]\n",
      "   [ 0.24313725  0.28235294  0.29019608]\n",
      "   [ 0.21960784  0.25882353  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.24313725  0.2745098   0.30196078]\n",
      "   [ 0.18823529  0.22352941  0.23137255]\n",
      "   [ 0.22352941  0.25882353  0.26666667]]\n",
      "\n",
      "  [[ 0.20392157  0.24313725  0.23921569]\n",
      "   [ 0.23529412  0.27058824  0.27843137]\n",
      "   [ 0.16862745  0.20392157  0.20392157]\n",
      "   ..., \n",
      "   [ 0.25882353  0.29803922  0.30980392]\n",
      "   [ 0.21568627  0.25882353  0.2627451 ]\n",
      "   [ 0.23529412  0.2745098   0.29019608]]\n",
      "\n",
      "  [[ 0.19607843  0.22745098  0.23137255]\n",
      "   [ 0.2         0.21960784  0.21960784]\n",
      "   [ 0.20784314  0.23529412  0.23921569]\n",
      "   ..., \n",
      "   [ 0.18039216  0.21176471  0.21176471]\n",
      "   [ 0.22745098  0.25882353  0.26666667]\n",
      "   [ 0.23921569  0.28235294  0.29019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.76862745  0.67058824  0.51372549]\n",
      "   [ 0.76862745  0.6745098   0.51372549]\n",
      "   [ 0.75686275  0.65882353  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69411765  0.6         0.50980392]\n",
      "   [ 0.67058824  0.57647059  0.48627451]\n",
      "   [ 0.64705882  0.55294118  0.45882353]]\n",
      "\n",
      "  [[ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.76470588  0.6627451   0.50588235]\n",
      "   ..., \n",
      "   [ 0.69803922  0.60784314  0.51764706]\n",
      "   [ 0.6745098   0.58431373  0.49411765]\n",
      "   [ 0.65490196  0.56470588  0.47058824]]\n",
      "\n",
      "  [[ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.75294118  0.65490196  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69019608  0.60392157  0.51764706]\n",
      "   [ 0.66666667  0.58431373  0.49411765]\n",
      "   [ 0.64705882  0.56078431  0.4745098 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.7372549   0.61960784  0.40392157]\n",
      "   [ 0.7372549   0.62745098  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.4       ]\n",
      "   ..., \n",
      "   [ 0.8         0.74509804  0.69803922]\n",
      "   [ 0.78431373  0.74117647  0.70196078]\n",
      "   [ 0.76862745  0.71764706  0.68235294]]\n",
      "\n",
      "  [[ 0.74117647  0.62352941  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.40784314]\n",
      "   [ 0.74509804  0.63137255  0.4       ]\n",
      "   ..., \n",
      "   [ 0.80392157  0.74509804  0.68627451]\n",
      "   [ 0.78431373  0.73333333  0.69019608]\n",
      "   [ 0.77254902  0.71764706  0.67843137]]\n",
      "\n",
      "  [[ 0.72156863  0.60392157  0.39215686]\n",
      "   [ 0.72156863  0.61176471  0.39215686]\n",
      "   [ 0.72156863  0.60784314  0.38039216]\n",
      "   ..., \n",
      "   [ 0.78039216  0.71372549  0.64705882]\n",
      "   [ 0.76862745  0.70588235  0.65490196]\n",
      "   [ 0.75686275  0.69411765  0.65098039]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  2, CIFAR-10 Batch 2:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.8         0.79607843  0.67843137]\n",
      "   [ 0.81568627  0.81960784  0.70980392]\n",
      "   [ 0.83921569  0.84313725  0.7372549 ]\n",
      "   ..., \n",
      "   [ 0.61176471  0.55294118  0.3254902 ]\n",
      "   [ 0.59215686  0.5372549   0.31764706]\n",
      "   [ 0.56862745  0.51764706  0.30588235]]\n",
      "\n",
      "  [[ 0.84705882  0.85098039  0.74901961]\n",
      "   [ 0.8745098   0.88235294  0.79215686]\n",
      "   [ 0.89019608  0.89411765  0.81176471]\n",
      "   ..., \n",
      "   [ 0.68235294  0.63529412  0.38823529]\n",
      "   [ 0.67843137  0.62745098  0.38039216]\n",
      "   [ 0.66666667  0.61176471  0.37254902]]\n",
      "\n",
      "  [[ 0.83921569  0.83137255  0.72941176]\n",
      "   [ 0.85490196  0.85098039  0.75294118]\n",
      "   [ 0.85490196  0.84705882  0.74117647]\n",
      "   ..., \n",
      "   [ 0.7372549   0.68627451  0.44313725]\n",
      "   [ 0.72941176  0.68235294  0.43921569]\n",
      "   [ 0.71372549  0.66666667  0.42352941]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.64313725  0.64705882  0.47843137]\n",
      "   [ 0.65882353  0.66666667  0.49019608]\n",
      "   [ 0.67058824  0.67843137  0.50196078]\n",
      "   ..., \n",
      "   [ 0.36862745  0.26666667  0.14509804]\n",
      "   [ 0.3372549   0.25098039  0.14117647]\n",
      "   [ 0.31372549  0.23921569  0.1372549 ]]\n",
      "\n",
      "  [[ 0.50196078  0.50196078  0.37647059]\n",
      "   [ 0.59607843  0.60392157  0.43921569]\n",
      "   [ 0.63921569  0.65098039  0.46666667]\n",
      "   ..., \n",
      "   [ 0.36470588  0.25882353  0.14117647]\n",
      "   [ 0.33333333  0.24705882  0.13333333]\n",
      "   [ 0.30980392  0.23137255  0.13333333]]\n",
      "\n",
      "  [[ 0.29411765  0.28235294  0.22745098]\n",
      "   [ 0.39607843  0.38431373  0.29803922]\n",
      "   [ 0.50588235  0.50588235  0.37254902]\n",
      "   ..., \n",
      "   [ 0.34901961  0.23921569  0.12941176]\n",
      "   [ 0.32156863  0.22745098  0.1254902 ]\n",
      "   [ 0.29411765  0.21176471  0.1254902 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.8         0.78431373  0.78431373]\n",
      "   [ 0.87843137  0.8627451   0.87058824]\n",
      "   [ 0.86666667  0.84705882  0.87058824]\n",
      "   ..., \n",
      "   [ 0.78039216  0.80392157  0.79215686]\n",
      "   [ 0.76470588  0.79215686  0.76862745]\n",
      "   [ 0.38823529  0.41568627  0.38431373]]\n",
      "\n",
      "  [[ 0.57254902  0.55686275  0.55686275]\n",
      "   [ 0.74901961  0.73333333  0.74117647]\n",
      "   [ 0.83529412  0.81568627  0.83921569]\n",
      "   ..., \n",
      "   [ 0.83921569  0.85882353  0.86666667]\n",
      "   [ 0.76470588  0.8         0.78823529]\n",
      "   [ 0.42745098  0.49803922  0.45490196]]\n",
      "\n",
      "  [[ 0.52941176  0.51372549  0.51372549]\n",
      "   [ 0.71372549  0.69411765  0.70196078]\n",
      "   [ 0.84705882  0.82745098  0.84705882]\n",
      "   ..., \n",
      "   [ 0.83529412  0.85098039  0.8745098 ]\n",
      "   [ 0.75294118  0.78823529  0.79607843]\n",
      "   [ 0.44705882  0.51764706  0.50196078]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.42745098  0.51372549  0.56470588]\n",
      "   [ 0.44313725  0.54117647  0.59215686]\n",
      "   [ 0.43921569  0.54117647  0.59215686]\n",
      "   ..., \n",
      "   [ 0.42745098  0.54117647  0.58431373]\n",
      "   [ 0.41568627  0.53333333  0.57647059]\n",
      "   [ 0.40784314  0.5254902   0.56862745]]\n",
      "\n",
      "  [[ 0.40784314  0.49803922  0.54901961]\n",
      "   [ 0.41960784  0.51764706  0.56862745]\n",
      "   [ 0.42745098  0.52941176  0.58039216]\n",
      "   ..., \n",
      "   [ 0.41960784  0.53333333  0.58039216]\n",
      "   [ 0.4         0.51764706  0.56078431]\n",
      "   [ 0.37647059  0.49411765  0.5372549 ]]\n",
      "\n",
      "  [[ 0.36470588  0.45490196  0.50980392]\n",
      "   [ 0.38823529  0.48627451  0.5372549 ]\n",
      "   [ 0.39607843  0.50196078  0.55294118]\n",
      "   ..., \n",
      "   [ 0.39215686  0.50588235  0.55294118]\n",
      "   [ 0.38823529  0.50588235  0.55294118]\n",
      "   [ 0.36862745  0.49019608  0.53333333]]]\n",
      "\n",
      "\n",
      " [[[ 0.41568627  0.3372549   0.61176471]\n",
      "   [ 0.48235294  0.4         0.50588235]\n",
      "   [ 0.43529412  0.33333333  0.49019608]\n",
      "   ..., \n",
      "   [ 0.51372549  0.4627451   0.61568627]\n",
      "   [ 0.42745098  0.39215686  0.55686275]\n",
      "   [ 0.55294118  0.52941176  0.61960784]]\n",
      "\n",
      "  [[ 0.60784314  0.5254902   0.63529412]\n",
      "   [ 0.6745098   0.57254902  0.44313725]\n",
      "   [ 0.51372549  0.43529412  0.48627451]\n",
      "   ..., \n",
      "   [ 0.51764706  0.46666667  0.60784314]\n",
      "   [ 0.39215686  0.34901961  0.50980392]\n",
      "   [ 0.56862745  0.5372549   0.63529412]]\n",
      "\n",
      "  [[ 0.63529412  0.5254902   0.68235294]\n",
      "   [ 0.62745098  0.49803922  0.43137255]\n",
      "   [ 0.44313725  0.34509804  0.5254902 ]\n",
      "   ..., \n",
      "   [ 0.73333333  0.71372549  0.65490196]\n",
      "   [ 0.69411765  0.6745098   0.65882353]\n",
      "   [ 0.78823529  0.77254902  0.71764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35686275  0.03921569  0.5254902 ]\n",
      "   [ 0.34509804  0.03529412  0.49803922]\n",
      "   [ 0.34509804  0.05098039  0.49803922]\n",
      "   ..., \n",
      "   [ 0.40784314  0.24705882  0.49411765]\n",
      "   [ 0.43529412  0.28235294  0.52156863]\n",
      "   [ 0.43921569  0.25490196  0.53333333]]\n",
      "\n",
      "  [[ 0.29803922  0.02352941  0.50196078]\n",
      "   [ 0.30588235  0.01568627  0.47058824]\n",
      "   [ 0.35686275  0.08235294  0.51764706]\n",
      "   ..., \n",
      "   [ 0.42745098  0.27058824  0.49803922]\n",
      "   [ 0.41960784  0.26666667  0.50980392]\n",
      "   [ 0.44705882  0.24705882  0.54509804]]\n",
      "\n",
      "  [[ 0.2627451   0.03137255  0.50196078]\n",
      "   [ 0.26666667  0.02352941  0.47058824]\n",
      "   [ 0.30980392  0.07058824  0.49803922]\n",
      "   ..., \n",
      "   [ 0.45882353  0.28627451  0.5254902 ]\n",
      "   [ 0.43137255  0.25098039  0.52156863]\n",
      "   [ 0.38823529  0.16470588  0.49019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.96862745  0.96470588  0.97254902]\n",
      "   [ 0.96078431  0.97647059  0.98823529]\n",
      "   [ 0.95686275  0.97254902  0.97647059]\n",
      "   ..., \n",
      "   [ 0.45882353  0.38039216  0.32941176]\n",
      "   [ 0.49803922  0.41960784  0.37647059]\n",
      "   [ 0.61960784  0.56470588  0.54117647]]\n",
      "\n",
      "  [[ 0.95294118  0.95686275  0.96862745]\n",
      "   [ 0.95294118  0.96862745  0.97647059]\n",
      "   [ 0.95294118  0.96078431  0.95686275]\n",
      "   ..., \n",
      "   [ 0.44313725  0.35686275  0.29803922]\n",
      "   [ 0.47843137  0.4         0.34509804]\n",
      "   [ 0.63137255  0.56862745  0.52941176]]\n",
      "\n",
      "  [[ 0.95686275  0.96078431  0.97647059]\n",
      "   [ 0.96078431  0.97647059  0.98039216]\n",
      "   [ 0.96862745  0.96862745  0.95686275]\n",
      "   ..., \n",
      "   [ 0.52941176  0.42745098  0.36470588]\n",
      "   [ 0.4745098   0.37647059  0.31764706]\n",
      "   [ 0.50588235  0.43137255  0.38823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.71372549  0.6627451   0.63529412]\n",
      "   [ 0.77647059  0.7254902   0.69803922]\n",
      "   [ 0.85490196  0.80392157  0.77647059]\n",
      "   ..., \n",
      "   [ 0.61568627  0.54901961  0.44313725]\n",
      "   [ 0.36862745  0.30588235  0.21960784]\n",
      "   [ 0.50980392  0.45882353  0.41176471]]\n",
      "\n",
      "  [[ 0.54509804  0.4627451   0.41960784]\n",
      "   [ 0.49411765  0.40392157  0.35686275]\n",
      "   [ 0.5254902   0.43529412  0.38431373]\n",
      "   ..., \n",
      "   [ 0.61960784  0.55686275  0.43921569]\n",
      "   [ 0.46666667  0.40784314  0.31372549]\n",
      "   [ 0.45490196  0.40784314  0.35294118]]\n",
      "\n",
      "  [[ 0.76862745  0.70196078  0.62745098]\n",
      "   [ 0.7254902   0.63921569  0.54509804]\n",
      "   [ 0.69019608  0.58823529  0.48235294]\n",
      "   ..., \n",
      "   [ 0.59607843  0.54901961  0.42352941]\n",
      "   [ 0.69411765  0.64313725  0.5372549 ]\n",
      "   [ 0.63529412  0.59607843  0.52156863]]]\n",
      "\n",
      "\n",
      " [[[ 0.74117647  0.89803922  0.94117647]\n",
      "   [ 0.76470588  0.90980392  0.94901961]\n",
      "   [ 0.79607843  0.93333333  0.96470588]\n",
      "   ..., \n",
      "   [ 0.65882353  0.75686275  0.81176471]\n",
      "   [ 0.65882353  0.74901961  0.79215686]\n",
      "   [ 0.65882353  0.7372549   0.78039216]]\n",
      "\n",
      "  [[ 0.79607843  0.9372549   0.96470588]\n",
      "   [ 0.83137255  0.96470588  0.98431373]\n",
      "   [ 0.82352941  0.94901961  0.96862745]\n",
      "   ..., \n",
      "   [ 0.58431373  0.67843137  0.76470588]\n",
      "   [ 0.59215686  0.6745098   0.76470588]\n",
      "   [ 0.60784314  0.67843137  0.77254902]]\n",
      "\n",
      "  [[ 0.83529412  0.96862745  0.98431373]\n",
      "   [ 0.81568627  0.94509804  0.96078431]\n",
      "   [ 0.82745098  0.94509804  0.95686275]\n",
      "   ..., \n",
      "   [ 0.58431373  0.6745098   0.76862745]\n",
      "   [ 0.57647059  0.65490196  0.76470588]\n",
      "   [ 0.56470588  0.63137255  0.75686275]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.31764706  0.35294118]\n",
      "   [ 0.21176471  0.32156863  0.47843137]\n",
      "   [ 0.15294118  0.40784314  0.62352941]\n",
      "   ..., \n",
      "   [ 0.18039216  0.15294118  0.18039216]\n",
      "   [ 0.19607843  0.16862745  0.2       ]\n",
      "   [ 0.27058824  0.24705882  0.25098039]]\n",
      "\n",
      "  [[ 0.25490196  0.2627451   0.28627451]\n",
      "   [ 0.16470588  0.22745098  0.35686275]\n",
      "   [ 0.17647059  0.36078431  0.50980392]\n",
      "   ..., \n",
      "   [ 0.14509804  0.08627451  0.17647059]\n",
      "   [ 0.17254902  0.10588235  0.18431373]\n",
      "   [ 0.22352941  0.17254902  0.2       ]]\n",
      "\n",
      "  [[ 0.20784314  0.22745098  0.24705882]\n",
      "   [ 0.15294118  0.2         0.25490196]\n",
      "   [ 0.24705882  0.30588235  0.36862745]\n",
      "   ..., \n",
      "   [ 0.18039216  0.10196078  0.19215686]\n",
      "   [ 0.23137255  0.14901961  0.18431373]\n",
      "   [ 0.29411765  0.23921569  0.2       ]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  2, CIFAR-10 Batch 3:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.00392157  0.00392157]\n",
      "   ..., \n",
      "   [ 0.03137255  0.03137255  0.01176471]\n",
      "   [ 0.03529412  0.03529412  0.01568627]\n",
      "   [ 0.03921569  0.02745098  0.01176471]]\n",
      "\n",
      "  [[ 0.00392157  0.00392157  0.00392157]\n",
      "   [ 0.00784314  0.00392157  0.00392157]\n",
      "   [ 0.00784314  0.00784314  0.00392157]\n",
      "   ..., \n",
      "   [ 0.02745098  0.02745098  0.01176471]\n",
      "   [ 0.03921569  0.03529412  0.01568627]\n",
      "   [ 0.03921569  0.03137255  0.01568627]]\n",
      "\n",
      "  [[ 0.02745098  0.01176471  0.00784314]\n",
      "   [ 0.02352941  0.01176471  0.00392157]\n",
      "   [ 0.03137255  0.01568627  0.00784314]\n",
      "   ..., \n",
      "   [ 0.03137255  0.02745098  0.01568627]\n",
      "   [ 0.04313725  0.03921569  0.01960784]\n",
      "   [ 0.03921569  0.03529412  0.01960784]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.32941176  0.2627451   0.19215686]\n",
      "   [ 0.35686275  0.29411765  0.21960784]\n",
      "   [ 0.37647059  0.31372549  0.23529412]\n",
      "   ..., \n",
      "   [ 0.49411765  0.41568627  0.3254902 ]\n",
      "   [ 0.50588235  0.42352941  0.3372549 ]\n",
      "   [ 0.48627451  0.40784314  0.32941176]]\n",
      "\n",
      "  [[ 0.32156863  0.26666667  0.2       ]\n",
      "   [ 0.34901961  0.29411765  0.21960784]\n",
      "   [ 0.36470588  0.30980392  0.23529412]\n",
      "   ..., \n",
      "   [ 0.48627451  0.40784314  0.32156863]\n",
      "   [ 0.50196078  0.41960784  0.3372549 ]\n",
      "   [ 0.49019608  0.41960784  0.3372549 ]]\n",
      "\n",
      "  [[ 0.30588235  0.2627451   0.19607843]\n",
      "   [ 0.3372549   0.29019608  0.21960784]\n",
      "   [ 0.35294118  0.30588235  0.23137255]\n",
      "   ..., \n",
      "   [ 0.48235294  0.41176471  0.31764706]\n",
      "   [ 0.49411765  0.42352941  0.33333333]\n",
      "   [ 0.48627451  0.41176471  0.33333333]]]\n",
      "\n",
      "\n",
      " [[[ 0.37647059  0.34117647  0.29803922]\n",
      "   [ 0.41960784  0.38431373  0.34509804]\n",
      "   [ 0.45490196  0.41176471  0.37647059]\n",
      "   ..., \n",
      "   [ 0.8627451   0.76470588  0.68627451]\n",
      "   [ 0.85882353  0.75686275  0.67843137]\n",
      "   [ 0.81960784  0.74117647  0.66666667]]\n",
      "\n",
      "  [[ 0.39607843  0.35686275  0.32156863]\n",
      "   [ 0.43529412  0.4         0.36470588]\n",
      "   [ 0.46666667  0.43137255  0.38823529]\n",
      "   ..., \n",
      "   [ 0.84705882  0.77254902  0.69411765]\n",
      "   [ 0.85098039  0.77647059  0.69803922]\n",
      "   [ 0.84313725  0.75294118  0.6627451 ]]\n",
      "\n",
      "  [[ 0.43529412  0.39215686  0.35686275]\n",
      "   [ 0.48235294  0.43529412  0.4       ]\n",
      "   [ 0.51764706  0.47058824  0.43529412]\n",
      "   ..., \n",
      "   [ 0.87843137  0.8         0.71764706]\n",
      "   [ 0.88627451  0.80784314  0.72156863]\n",
      "   [ 0.87058824  0.78431373  0.69411765]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.55686275  0.62352941  0.62352941]\n",
      "   [ 0.56862745  0.63529412  0.63921569]\n",
      "   [ 0.54509804  0.61176471  0.61176471]\n",
      "   ..., \n",
      "   [ 0.71764706  0.6627451   0.65882353]\n",
      "   [ 0.69411765  0.61176471  0.56862745]\n",
      "   [ 0.6745098   0.60392157  0.55294118]]\n",
      "\n",
      "  [[ 0.56078431  0.63137255  0.62745098]\n",
      "   [ 0.56862745  0.63921569  0.63921569]\n",
      "   [ 0.55686275  0.62352941  0.62745098]\n",
      "   ..., \n",
      "   [ 0.76470588  0.70980392  0.70196078]\n",
      "   [ 0.75294118  0.65490196  0.6       ]\n",
      "   [ 0.75294118  0.66666667  0.60784314]]\n",
      "\n",
      "  [[ 0.54901961  0.62352941  0.61960784]\n",
      "   [ 0.56470588  0.64313725  0.64313725]\n",
      "   [ 0.55686275  0.62745098  0.63137255]\n",
      "   ..., \n",
      "   [ 0.80392157  0.81568627  0.85098039]\n",
      "   [ 0.83137255  0.78039216  0.75294118]\n",
      "   [ 0.81568627  0.76078431  0.70980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.6745098   0.67843137  0.59607843]\n",
      "   [ 0.67058824  0.6745098   0.59607843]\n",
      "   [ 0.69803922  0.69803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.41960784  0.41176471  0.30196078]\n",
      "   [ 0.41568627  0.40392157  0.30980392]\n",
      "   [ 0.4         0.38823529  0.30588235]]\n",
      "\n",
      "  [[ 0.64705882  0.63921569  0.56078431]\n",
      "   [ 0.62352941  0.61568627  0.54117647]\n",
      "   [ 0.70588235  0.69803922  0.62352941]\n",
      "   ..., \n",
      "   [ 0.45882353  0.43921569  0.3254902 ]\n",
      "   [ 0.45882353  0.43921569  0.3372549 ]\n",
      "   [ 0.43529412  0.41568627  0.3254902 ]]\n",
      "\n",
      "  [[ 0.68235294  0.6627451   0.58823529]\n",
      "   [ 0.61176471  0.59215686  0.51764706]\n",
      "   [ 0.68235294  0.6627451   0.58823529]\n",
      "   ..., \n",
      "   [ 0.47058824  0.44705882  0.3254902 ]\n",
      "   [ 0.4745098   0.44705882  0.3372549 ]\n",
      "   [ 0.46666667  0.43921569  0.34509804]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.47843137  0.45882353  0.36862745]\n",
      "   [ 0.47058824  0.45098039  0.36470588]\n",
      "   [ 0.45490196  0.43921569  0.34901961]\n",
      "   ..., \n",
      "   [ 0.48627451  0.49803922  0.39215686]\n",
      "   [ 0.4745098   0.49411765  0.39215686]\n",
      "   [ 0.45882353  0.47843137  0.38039216]]\n",
      "\n",
      "  [[ 0.43529412  0.41176471  0.30588235]\n",
      "   [ 0.43921569  0.41960784  0.3254902 ]\n",
      "   [ 0.48235294  0.48627451  0.39607843]\n",
      "   ..., \n",
      "   [ 0.47843137  0.45490196  0.32941176]\n",
      "   [ 0.45882353  0.4627451   0.35294118]\n",
      "   [ 0.44313725  0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.43921569  0.41176471  0.30196078]\n",
      "   [ 0.45098039  0.42352941  0.32941176]\n",
      "   [ 0.4627451   0.47058824  0.37647059]\n",
      "   ..., \n",
      "   [ 0.48627451  0.45098039  0.31372549]\n",
      "   [ 0.43137255  0.43137255  0.31764706]\n",
      "   [ 0.4         0.41568627  0.30980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.37254902  0.34509804  0.2       ]\n",
      "   [ 0.36078431  0.34509804  0.19215686]\n",
      "   [ 0.35686275  0.34117647  0.19607843]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20392157  0.11764706]\n",
      "   [ 0.20392157  0.2         0.11764706]\n",
      "   [ 0.21176471  0.21176471  0.1254902 ]]\n",
      "\n",
      "  [[ 0.4         0.36862745  0.20784314]\n",
      "   [ 0.41568627  0.38039216  0.2       ]\n",
      "   [ 0.41960784  0.38431373  0.21176471]\n",
      "   ..., \n",
      "   [ 0.20392157  0.20392157  0.1254902 ]\n",
      "   [ 0.19215686  0.18823529  0.10588235]\n",
      "   [ 0.23137255  0.22745098  0.1254902 ]]\n",
      "\n",
      "  [[ 0.41960784  0.38431373  0.20392157]\n",
      "   [ 0.42745098  0.38823529  0.20392157]\n",
      "   [ 0.41176471  0.38039216  0.20392157]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20784314  0.1254902 ]\n",
      "   [ 0.21568627  0.20784314  0.11764706]\n",
      "   [ 0.24313725  0.23529412  0.1254902 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.37647059  0.36078431  0.23529412]\n",
      "   [ 0.24313725  0.23921569  0.15686275]\n",
      "   [ 0.20784314  0.21176471  0.14117647]\n",
      "   ..., \n",
      "   [ 0.19607843  0.19607843  0.12941176]\n",
      "   [ 0.21568627  0.21568627  0.14117647]\n",
      "   [ 0.23529412  0.22745098  0.15294118]]\n",
      "\n",
      "  [[ 0.31372549  0.29803922  0.2       ]\n",
      "   [ 0.23137255  0.22745098  0.15294118]\n",
      "   [ 0.19607843  0.2         0.12941176]\n",
      "   ..., \n",
      "   [ 0.21960784  0.20392157  0.13333333]\n",
      "   [ 0.25098039  0.23529412  0.15294118]\n",
      "   [ 0.25098039  0.23529412  0.14901961]]\n",
      "\n",
      "  [[ 0.18431373  0.18039216  0.1254902 ]\n",
      "   [ 0.15294118  0.15686275  0.10980392]\n",
      "   [ 0.15294118  0.15686275  0.09803922]\n",
      "   ..., \n",
      "   [ 0.30980392  0.29019608  0.17647059]\n",
      "   [ 0.25098039  0.23137255  0.14901961]\n",
      "   [ 0.23921569  0.22352941  0.14901961]]]\n",
      "\n",
      "\n",
      " [[[ 0.60784314  0.40392157  0.43137255]\n",
      "   [ 0.59607843  0.39215686  0.41960784]\n",
      "   [ 0.60784314  0.40392157  0.43137255]\n",
      "   ..., \n",
      "   [ 0.23137255  0.14901961  0.17254902]\n",
      "   [ 0.22352941  0.15686275  0.18039216]\n",
      "   [ 0.22352941  0.16078431  0.19607843]]\n",
      "\n",
      "  [[ 0.60784314  0.40784314  0.43529412]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   [ 0.6         0.4         0.42745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.18823529  0.20784314]\n",
      "   [ 0.21960784  0.15294118  0.17647059]\n",
      "   [ 0.21960784  0.16078431  0.19215686]]\n",
      "\n",
      "  [[ 0.61176471  0.41176471  0.43921569]\n",
      "   [ 0.58823529  0.38823529  0.41568627]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   ..., \n",
      "   [ 0.41176471  0.30980392  0.3254902 ]\n",
      "   [ 0.23137255  0.16470588  0.18823529]\n",
      "   [ 0.2         0.14117647  0.17254902]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.45490196  0.43921569  0.5254902 ]\n",
      "   [ 0.44313725  0.43529412  0.50980392]\n",
      "   [ 0.44313725  0.44313725  0.50196078]\n",
      "   ..., \n",
      "   [ 0.42745098  0.38039216  0.44705882]\n",
      "   [ 0.28235294  0.23921569  0.29803922]\n",
      "   [ 0.42352941  0.37647059  0.43529412]]\n",
      "\n",
      "  [[ 0.45490196  0.44313725  0.52156863]\n",
      "   [ 0.44705882  0.43529412  0.50980392]\n",
      "   [ 0.45098039  0.43921569  0.51372549]\n",
      "   ..., \n",
      "   [ 0.43137255  0.41176471  0.48627451]\n",
      "   [ 0.23137255  0.19607843  0.26666667]\n",
      "   [ 0.29411765  0.23921569  0.30980392]]\n",
      "\n",
      "  [[ 0.46666667  0.45490196  0.52941176]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   ..., \n",
      "   [ 0.47058824  0.45882353  0.5372549 ]\n",
      "   [ 0.39607843  0.37254902  0.44705882]\n",
      "   [ 0.24705882  0.19607843  0.26666667]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  2, CIFAR-10 Batch 4:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.58823529  0.54117647  0.51764706]\n",
      "   [ 0.58039216  0.5372549   0.51372549]\n",
      "   [ 0.60392157  0.56078431  0.5372549 ]\n",
      "   ..., \n",
      "   [ 0.60784314  0.56078431  0.52941176]\n",
      "   [ 0.64705882  0.58823529  0.55686275]\n",
      "   [ 0.65882353  0.58823529  0.56078431]]\n",
      "\n",
      "  [[ 0.56862745  0.5254902   0.50196078]\n",
      "   [ 0.54509804  0.50196078  0.47843137]\n",
      "   [ 0.57254902  0.52941176  0.50588235]\n",
      "   ..., \n",
      "   [ 0.60784314  0.56078431  0.52941176]\n",
      "   [ 0.64313725  0.58431373  0.55686275]\n",
      "   [ 0.69019608  0.62352941  0.59215686]]\n",
      "\n",
      "  [[ 0.57254902  0.52941176  0.50588235]\n",
      "   [ 0.56862745  0.5254902   0.50196078]\n",
      "   [ 0.56862745  0.5254902   0.50196078]\n",
      "   ..., \n",
      "   [ 0.61176471  0.56470588  0.53333333]\n",
      "   [ 0.63137255  0.57254902  0.54117647]\n",
      "   [ 0.6745098   0.60784314  0.58039216]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.55294118  0.52941176  0.48235294]\n",
      "   [ 0.49803922  0.4745098   0.42745098]\n",
      "   [ 0.52156863  0.49803922  0.45098039]\n",
      "   ..., \n",
      "   [ 0.42352941  0.42352941  0.38431373]\n",
      "   [ 0.4         0.4         0.36078431]\n",
      "   [ 0.36862745  0.36470588  0.3254902 ]]\n",
      "\n",
      "  [[ 0.49803922  0.4745098   0.42352941]\n",
      "   [ 0.49019608  0.46666667  0.41960784]\n",
      "   [ 0.51764706  0.49411765  0.44705882]\n",
      "   ..., \n",
      "   [ 0.30980392  0.30980392  0.27058824]\n",
      "   [ 0.25490196  0.25490196  0.21568627]\n",
      "   [ 0.26666667  0.26666667  0.22745098]]\n",
      "\n",
      "  [[ 0.54901961  0.5254902   0.47843137]\n",
      "   [ 0.47058824  0.44705882  0.4       ]\n",
      "   [ 0.50196078  0.47843137  0.43137255]\n",
      "   ..., \n",
      "   [ 0.25490196  0.25490196  0.21568627]\n",
      "   [ 0.25882353  0.25882353  0.21960784]\n",
      "   [ 0.2745098   0.2745098   0.23529412]]]\n",
      "\n",
      "\n",
      " [[[ 0.6         0.59607843  0.43137255]\n",
      "   [ 0.61176471  0.60784314  0.44313725]\n",
      "   [ 0.58823529  0.58431373  0.41960784]\n",
      "   ..., \n",
      "   [ 0.60784314  0.59607843  0.41960784]\n",
      "   [ 0.57647059  0.56470588  0.38431373]\n",
      "   [ 0.58039216  0.57647059  0.39607843]]\n",
      "\n",
      "  [[ 0.57254902  0.55686275  0.39607843]\n",
      "   [ 0.60784314  0.59215686  0.44313725]\n",
      "   [ 0.60392157  0.58823529  0.44705882]\n",
      "   ..., \n",
      "   [ 0.64313725  0.63137255  0.45490196]\n",
      "   [ 0.58039216  0.56862745  0.39607843]\n",
      "   [ 0.56862745  0.56078431  0.39215686]]\n",
      "\n",
      "  [[ 0.59215686  0.56470588  0.40784314]\n",
      "   [ 0.60392157  0.57254902  0.43529412]\n",
      "   [ 0.59215686  0.55686275  0.43921569]\n",
      "   ..., \n",
      "   [ 0.63529412  0.62352941  0.44705882]\n",
      "   [ 0.61568627  0.60392157  0.42745098]\n",
      "   [ 0.56862745  0.56078431  0.4       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.70196078  0.70980392  0.61568627]\n",
      "   [ 0.67058824  0.67058824  0.53333333]\n",
      "   [ 0.66666667  0.65490196  0.50196078]\n",
      "   ..., \n",
      "   [ 0.66666667  0.65490196  0.49411765]\n",
      "   [ 0.65882353  0.64313725  0.48627451]\n",
      "   [ 0.65490196  0.62352941  0.47843137]]\n",
      "\n",
      "  [[ 0.70980392  0.70980392  0.61176471]\n",
      "   [ 0.6627451   0.6627451   0.5254902 ]\n",
      "   [ 0.6627451   0.65490196  0.50588235]\n",
      "   ..., \n",
      "   [ 0.6627451   0.65098039  0.49019608]\n",
      "   [ 0.67058824  0.65490196  0.49803922]\n",
      "   [ 0.66666667  0.64705882  0.50196078]]\n",
      "\n",
      "  [[ 0.69803922  0.68627451  0.57647059]\n",
      "   [ 0.66666667  0.65882353  0.52156863]\n",
      "   [ 0.67058824  0.66666667  0.52156863]\n",
      "   ..., \n",
      "   [ 0.65098039  0.63921569  0.47843137]\n",
      "   [ 0.6627451   0.65490196  0.49411765]\n",
      "   [ 0.65490196  0.65490196  0.50980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.27058824  0.34509804  0.44705882]\n",
      "   [ 0.35294118  0.4745098   0.58823529]\n",
      "   [ 0.35294118  0.49803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.00784314  0.01176471  0.07058824]\n",
      "   [ 0.00784314  0.00784314  0.0627451 ]\n",
      "   [ 0.00784314  0.00784314  0.05882353]]\n",
      "\n",
      "  [[ 0.11372549  0.15294118  0.25098039]\n",
      "   [ 0.05882353  0.10588235  0.20784314]\n",
      "   [ 0.05098039  0.09803922  0.2       ]\n",
      "   ..., \n",
      "   [ 0.00392157  0.          0.00784314]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.00392157  0.00784314]]\n",
      "\n",
      "  [[ 0.01568627  0.01176471  0.01960784]\n",
      "   [ 0.01568627  0.00392157  0.00784314]\n",
      "   [ 0.01176471  0.00392157  0.00784314]\n",
      "   ..., \n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.00784314]\n",
      "   [ 0.          0.          0.01176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.00392157  0.00392157  0.01960784]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.        ]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03137255  0.04313725]\n",
      "   [ 0.00784314  0.00784314  0.01568627]\n",
      "   [ 0.00392157  0.00392157  0.01568627]]\n",
      "\n",
      "  [[ 0.03137255  0.0627451   0.12941176]\n",
      "   [ 0.00392157  0.01568627  0.05098039]\n",
      "   [ 0.          0.00392157  0.01176471]\n",
      "   ..., \n",
      "   [ 0.02745098  0.03921569  0.09019608]\n",
      "   [ 0.01176471  0.01568627  0.03921569]\n",
      "   [ 0.00392157  0.00392157  0.01176471]]\n",
      "\n",
      "  [[ 0.14509804  0.25098039  0.42352941]\n",
      "   [ 0.09411765  0.16862745  0.30196078]\n",
      "   [ 0.03921569  0.0745098   0.15294118]\n",
      "   ..., \n",
      "   [ 0.07843137  0.11764706  0.23921569]\n",
      "   [ 0.05098039  0.07843137  0.16862745]\n",
      "   [ 0.02352941  0.03921569  0.09019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.7254902   0.79215686  0.81176471]\n",
      "   [ 0.67843137  0.77647059  0.80392157]\n",
      "   [ 0.69019608  0.81176471  0.85098039]\n",
      "   ..., \n",
      "   [ 0.14509804  0.17647059  0.24313725]\n",
      "   [ 0.10196078  0.1372549   0.2       ]\n",
      "   [ 0.12941176  0.19607843  0.27058824]]\n",
      "\n",
      "  [[ 0.42745098  0.47058824  0.46666667]\n",
      "   [ 0.4627451   0.52941176  0.52941176]\n",
      "   [ 0.4745098   0.56078431  0.56470588]\n",
      "   ..., \n",
      "   [ 0.16862745  0.19607843  0.23529412]\n",
      "   [ 0.12941176  0.13333333  0.16862745]\n",
      "   [ 0.15686275  0.18823529  0.22745098]]\n",
      "\n",
      "  [[ 0.20392157  0.22745098  0.21568627]\n",
      "   [ 0.22745098  0.26666667  0.25098039]\n",
      "   [ 0.22745098  0.28235294  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15686275  0.18431373  0.22745098]\n",
      "   [ 0.18431373  0.22745098  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.63529412  0.62745098  0.6745098 ]\n",
      "   [ 0.61960784  0.61176471  0.65490196]\n",
      "   [ 0.62352941  0.61568627  0.65882353]\n",
      "   ..., \n",
      "   [ 0.05882353  0.05098039  0.09019608]\n",
      "   [ 0.05490196  0.04705882  0.09019608]\n",
      "   [ 0.07058824  0.0627451   0.10588235]]\n",
      "\n",
      "  [[ 0.61176471  0.60392157  0.65882353]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   ..., \n",
      "   [ 0.05098039  0.04313725  0.08235294]\n",
      "   [ 0.05882353  0.05098039  0.09411765]\n",
      "   [ 0.1254902   0.11764706  0.16078431]]\n",
      "\n",
      "  [[ 0.58431373  0.57647059  0.63137255]\n",
      "   [ 0.56470588  0.55686275  0.61176471]\n",
      "   [ 0.57254902  0.56470588  0.61960784]\n",
      "   ..., \n",
      "   [ 0.0627451   0.05490196  0.09411765]\n",
      "   [ 0.20392157  0.19607843  0.23921569]\n",
      "   [ 0.38431373  0.37647059  0.41960784]]]\n",
      "\n",
      "\n",
      " [[[ 0.76470588  0.71764706  0.67058824]\n",
      "   [ 0.75686275  0.70980392  0.6627451 ]\n",
      "   [ 0.76078431  0.71372549  0.66666667]\n",
      "   ..., \n",
      "   [ 0.22352941  0.22352941  0.22352941]\n",
      "   [ 0.20392157  0.20392157  0.20392157]\n",
      "   [ 0.03137255  0.03137255  0.03137255]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.67843137]\n",
      "   [ 0.76470588  0.71764706  0.6745098 ]\n",
      "   [ 0.77254902  0.7254902   0.68235294]\n",
      "   ..., \n",
      "   [ 0.34117647  0.34117647  0.34117647]\n",
      "   [ 0.31372549  0.31372549  0.31372549]\n",
      "   [ 0.03921569  0.03921569  0.03921569]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.68627451]\n",
      "   [ 0.76862745  0.71764706  0.68235294]\n",
      "   [ 0.77647059  0.7254902   0.69411765]\n",
      "   ..., \n",
      "   [ 0.43137255  0.43137255  0.43137255]\n",
      "   [ 0.41568627  0.41568627  0.41568627]\n",
      "   [ 0.04705882  0.04705882  0.04705882]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.78039216  0.7254902   0.69411765]\n",
      "   [ 0.76862745  0.72156863  0.68627451]\n",
      "   [ 0.78039216  0.74117647  0.69803922]\n",
      "   ..., \n",
      "   [ 0.13333333  0.10980392  0.08235294]\n",
      "   [ 0.10980392  0.09019608  0.07058824]\n",
      "   [ 0.08627451  0.0745098   0.0627451 ]]\n",
      "\n",
      "  [[ 0.77254902  0.7254902   0.69019608]\n",
      "   [ 0.76470588  0.71764706  0.68235294]\n",
      "   [ 0.77254902  0.72941176  0.69411765]\n",
      "   ..., \n",
      "   [ 0.15294118  0.12156863  0.08235294]\n",
      "   [ 0.14509804  0.12156863  0.08627451]\n",
      "   [ 0.1372549   0.11372549  0.08627451]]\n",
      "\n",
      "  [[ 0.75686275  0.71764706  0.68235294]\n",
      "   [ 0.75686275  0.70588235  0.6745098 ]\n",
      "   [ 0.76470588  0.70980392  0.67843137]\n",
      "   ..., \n",
      "   [ 0.16470588  0.12941176  0.08235294]\n",
      "   [ 0.16862745  0.12941176  0.09019608]\n",
      "   [ 0.16078431  0.1254902   0.09411765]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "Epoch  2, CIFAR-10 Batch 5:  session:<tensorflow.python.client.session.Session object at 0x127707390>\n",
      "feature_batch:[[[[ 0.05490196  0.06666667  0.11372549]\n",
      "   [ 0.05098039  0.05490196  0.10196078]\n",
      "   [ 0.04705882  0.07843137  0.11372549]\n",
      "   ..., \n",
      "   [ 0.09019608  0.19215686  0.17647059]\n",
      "   [ 0.08627451  0.17647059  0.14901961]\n",
      "   [ 0.07843137  0.16078431  0.13333333]]\n",
      "\n",
      "  [[ 0.09411765  0.11372549  0.16078431]\n",
      "   [ 0.09803922  0.14117647  0.1254902 ]\n",
      "   [ 0.06666667  0.1254902   0.10588235]\n",
      "   ..., \n",
      "   [ 0.0627451   0.16862745  0.13333333]\n",
      "   [ 0.10196078  0.20392157  0.15686275]\n",
      "   [ 0.09019608  0.18823529  0.14117647]]\n",
      "\n",
      "  [[ 0.13333333  0.16470588  0.19607843]\n",
      "   [ 0.10588235  0.17647059  0.1372549 ]\n",
      "   [ 0.10588235  0.18823529  0.14117647]\n",
      "   ..., \n",
      "   [ 0.09803922  0.21176471  0.16078431]\n",
      "   [ 0.10588235  0.22745098  0.15686275]\n",
      "   [ 0.12156863  0.23529412  0.16862745]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.79607843  0.78823529  0.78039216]\n",
      "   [ 0.78823529  0.78431373  0.76470588]\n",
      "   [ 0.79607843  0.78039216  0.78039216]\n",
      "   ..., \n",
      "   [ 0.3254902   0.3254902   0.22352941]\n",
      "   [ 0.34509804  0.38431373  0.29411765]\n",
      "   [ 0.35686275  0.35686275  0.28627451]]\n",
      "\n",
      "  [[ 0.81568627  0.8         0.78823529]\n",
      "   [ 0.81568627  0.8         0.78823529]\n",
      "   [ 0.81960784  0.80392157  0.79215686]\n",
      "   ..., \n",
      "   [ 0.40392157  0.38431373  0.28235294]\n",
      "   [ 0.25882353  0.30196078  0.21176471]\n",
      "   [ 0.25882353  0.28627451  0.21960784]]\n",
      "\n",
      "  [[ 0.85882353  0.84313725  0.82745098]\n",
      "   [ 0.85098039  0.83529412  0.82352941]\n",
      "   [ 0.85490196  0.83921569  0.82745098]\n",
      "   ..., \n",
      "   [ 0.42352941  0.40392157  0.31764706]\n",
      "   [ 0.41960784  0.41176471  0.3372549 ]\n",
      "   [ 0.29803922  0.33333333  0.25882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.56862745  0.54117647  0.44313725]\n",
      "   [ 0.70980392  0.68627451  0.63529412]\n",
      "   [ 0.72156863  0.71372549  0.6627451 ]\n",
      "   ..., \n",
      "   [ 0.78823529  0.79215686  0.82745098]\n",
      "   [ 0.77647059  0.78431373  0.81960784]\n",
      "   [ 0.78823529  0.79607843  0.83137255]]\n",
      "\n",
      "  [[ 0.43921569  0.39607843  0.20784314]\n",
      "   [ 0.55686275  0.51764706  0.34509804]\n",
      "   [ 0.54509804  0.51764706  0.31764706]\n",
      "   ..., \n",
      "   [ 0.74901961  0.77647059  0.81960784]\n",
      "   [ 0.74117647  0.77254902  0.81568627]\n",
      "   [ 0.76078431  0.79215686  0.83921569]]\n",
      "\n",
      "  [[ 0.40784314  0.34117647  0.14901961]\n",
      "   [ 0.40784314  0.34117647  0.14117647]\n",
      "   [ 0.38431373  0.3254902   0.07843137]\n",
      "   ..., \n",
      "   [ 0.74901961  0.79215686  0.83921569]\n",
      "   [ 0.75686275  0.8         0.84705882]\n",
      "   [ 0.77254902  0.81568627  0.8627451 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.6627451   0.49019608  0.34901961]\n",
      "   [ 0.63921569  0.46666667  0.32941176]\n",
      "   [ 0.63137255  0.45098039  0.31764706]\n",
      "   ..., \n",
      "   [ 0.19215686  0.16862745  0.16470588]\n",
      "   [ 0.16862745  0.14509804  0.14509804]\n",
      "   [ 0.16078431  0.13333333  0.13333333]]\n",
      "\n",
      "  [[ 0.72941176  0.56862745  0.39607843]\n",
      "   [ 0.7372549   0.55686275  0.38431373]\n",
      "   [ 0.70980392  0.53333333  0.38039216]\n",
      "   ..., \n",
      "   [ 0.21960784  0.19607843  0.2       ]\n",
      "   [ 0.19607843  0.17647059  0.18039216]\n",
      "   [ 0.18431373  0.16470588  0.16862745]]\n",
      "\n",
      "  [[ 0.67843137  0.5372549   0.40392157]\n",
      "   [ 0.74117647  0.54901961  0.38431373]\n",
      "   [ 0.65098039  0.49411765  0.35686275]\n",
      "   ..., \n",
      "   [ 0.17254902  0.16078431  0.16862745]\n",
      "   [ 0.15686275  0.14509804  0.15686275]\n",
      "   [ 0.14509804  0.1372549   0.14509804]]]\n",
      "\n",
      "\n",
      " [[[ 0.23921569  0.28627451  0.29803922]\n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15294118  0.19215686  0.25882353]\n",
      "   ..., \n",
      "   [ 0.27843137  0.35294118  0.31372549]\n",
      "   [ 0.24313725  0.30588235  0.29411765]\n",
      "   [ 0.17647059  0.22745098  0.23921569]]\n",
      "\n",
      "  [[ 0.24705882  0.29411765  0.30196078]\n",
      "   [ 0.17647059  0.22745098  0.2627451 ]\n",
      "   [ 0.1254902   0.16862745  0.22745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.34901961  0.32156863]\n",
      "   [ 0.27843137  0.32941176  0.31372549]\n",
      "   [ 0.19607843  0.23529412  0.24313725]]\n",
      "\n",
      "  [[ 0.24705882  0.31372549  0.30196078]\n",
      "   [ 0.22352941  0.29411765  0.29411765]\n",
      "   [ 0.24705882  0.30980392  0.31764706]\n",
      "   ..., \n",
      "   [ 0.32156863  0.37647059  0.35686275]\n",
      "   [ 0.29803922  0.34509804  0.32156863]\n",
      "   [ 0.20784314  0.24705882  0.24313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.51372549  0.52156863  0.4       ]\n",
      "   [ 0.61176471  0.60392157  0.4627451 ]\n",
      "   [ 0.63137255  0.61568627  0.4745098 ]\n",
      "   ..., \n",
      "   [ 0.91764706  0.8745098   0.67843137]\n",
      "   [ 0.8627451   0.8         0.61176471]\n",
      "   [ 0.70980392  0.65098039  0.49411765]]\n",
      "\n",
      "  [[ 0.41176471  0.42352941  0.3254902 ]\n",
      "   [ 0.41960784  0.42352941  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.32941176]\n",
      "   ..., \n",
      "   [ 0.92156863  0.86666667  0.6745098 ]\n",
      "   [ 0.89803922  0.81568627  0.63137255]\n",
      "   [ 0.74901961  0.66666667  0.51372549]]\n",
      "\n",
      "  [[ 0.2627451   0.29019608  0.23921569]\n",
      "   [ 0.30588235  0.3254902   0.26666667]\n",
      "   [ 0.43921569  0.45098039  0.35294118]\n",
      "   ..., \n",
      "   [ 0.89019608  0.80784314  0.58823529]\n",
      "   [ 0.85098039  0.75294118  0.54509804]\n",
      "   [ 0.72156863  0.62745098  0.45882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.02352941  0.05882353]\n",
      "   [ 0.07843137  0.08627451  0.09019608]\n",
      "   ..., \n",
      "   [ 0.23137255  0.27843137  0.21568627]\n",
      "   [ 0.22352941  0.2745098   0.21568627]\n",
      "   [ 0.20784314  0.26666667  0.23137255]]\n",
      "\n",
      "  [[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.03529412  0.05882353]\n",
      "   [ 0.09803922  0.1254902   0.10980392]\n",
      "   ..., \n",
      "   [ 0.21568627  0.23921569  0.18823529]\n",
      "   [ 0.25882353  0.29803922  0.22745098]\n",
      "   [ 0.18823529  0.24705882  0.21176471]]\n",
      "\n",
      "  [[ 0.04705882  0.02352941  0.0627451 ]\n",
      "   [ 0.04313725  0.03921569  0.05882353]\n",
      "   [ 0.14901961  0.18431373  0.14901961]\n",
      "   ..., \n",
      "   [ 0.18431373  0.20392157  0.16470588]\n",
      "   [ 0.22352941  0.25882353  0.19215686]\n",
      "   [ 0.20392157  0.25098039  0.2       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.69411765  0.6627451   0.71764706]\n",
      "   [ 0.70588235  0.6627451   0.72156863]\n",
      "   [ 0.72156863  0.6745098   0.74509804]\n",
      "   ..., \n",
      "   [ 0.6745098   0.62352941  0.68235294]\n",
      "   [ 0.66666667  0.61568627  0.67058824]\n",
      "   [ 0.64313725  0.58823529  0.64705882]]\n",
      "\n",
      "  [[ 0.62352941  0.61568627  0.69019608]\n",
      "   [ 0.63529412  0.61568627  0.69411765]\n",
      "   [ 0.65490196  0.63529412  0.71764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.69803922  0.78431373]\n",
      "   [ 0.70980392  0.68627451  0.77254902]\n",
      "   [ 0.69803922  0.67843137  0.76470588]]\n",
      "\n",
      "  [[ 0.61960784  0.61960784  0.72941176]\n",
      "   [ 0.62352941  0.62352941  0.73333333]\n",
      "   [ 0.63921569  0.63921569  0.74509804]\n",
      "   ..., \n",
      "   [ 0.70980392  0.70588235  0.82745098]\n",
      "   [ 0.70196078  0.69411765  0.81568627]\n",
      "   [ 0.68627451  0.68235294  0.80392157]]]\n",
      "\n",
      "\n",
      " [[[ 0.68627451  0.75686275  0.89803922]\n",
      "   [ 0.6745098   0.75294118  0.9254902 ]\n",
      "   [ 0.67058824  0.75686275  0.94117647]\n",
      "   ..., \n",
      "   [ 0.75686275  0.80784314  0.93333333]\n",
      "   [ 0.76862745  0.80784314  0.90588235]\n",
      "   [ 0.76078431  0.79607843  0.89019608]]\n",
      "\n",
      "  [[ 0.65490196  0.73333333  0.88627451]\n",
      "   [ 0.64313725  0.73333333  0.90196078]\n",
      "   [ 0.64313725  0.7372549   0.90980392]\n",
      "   ..., \n",
      "   [ 0.70196078  0.75686275  0.88235294]\n",
      "   [ 0.69803922  0.74901961  0.85098039]\n",
      "   [ 0.69019608  0.73333333  0.83137255]]\n",
      "\n",
      "  [[ 0.65490196  0.72156863  0.86666667]\n",
      "   [ 0.65490196  0.72941176  0.87058824]\n",
      "   [ 0.67058824  0.72941176  0.85098039]\n",
      "   ..., \n",
      "   [ 0.69019608  0.74901961  0.87843137]\n",
      "   [ 0.68235294  0.74117647  0.85098039]\n",
      "   [ 0.67058824  0.72156863  0.82745098]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.33333333  0.32941176  0.39607843]\n",
      "   [ 0.33333333  0.32156863  0.36470588]\n",
      "   [ 0.36078431  0.32941176  0.32156863]\n",
      "   ..., \n",
      "   [ 0.4745098   0.44313725  0.47058824]\n",
      "   [ 0.41960784  0.40392157  0.46666667]\n",
      "   [ 0.45882353  0.43921569  0.50588235]]\n",
      "\n",
      "  [[ 0.33333333  0.34117647  0.4       ]\n",
      "   [ 0.32941176  0.31764706  0.34901961]\n",
      "   [ 0.34117647  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.30196078  0.28235294  0.32941176]\n",
      "   [ 0.43137255  0.40392157  0.47058824]\n",
      "   [ 0.44705882  0.41960784  0.48627451]]\n",
      "\n",
      "  [[ 0.32156863  0.32941176  0.37647059]\n",
      "   [ 0.29411765  0.29019608  0.32156863]\n",
      "   [ 0.22352941  0.19607843  0.21568627]\n",
      "   ..., \n",
      "   [ 0.30196078  0.26666667  0.30588235]\n",
      "   [ 0.35686275  0.30588235  0.35294118]\n",
      "   [ 0.35686275  0.30588235  0.35294118]]]]\n",
      "label_batch:[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cost:Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "acc:Tensor(\"accuracy:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
